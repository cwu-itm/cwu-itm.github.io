<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <link href="css/styles.css" rel="stylesheet">
      <title>Nicole DeGregory | Portfolio</title>
   </head>
   <body>
<!-- Page Header  -->
      <header class="page-header">
         <h1>Portfolio</h1>
      </header>
<!-- Main Content Area  -->
      <main>
          <!-- Module 1.1 Blog Post Starts Here-->
          <article>
              <header class="article-header">
                  <h2>Module 1.1 Blog Post</h2>
              </header>
              <h3>Systems Thinking in Digital Design</h3>
              <p>Hey everyone! For my first post, I'm exploring systems thinking—a foundational concept in systems engineering and digital environments. In our world today, the things we build aren't just standalone tools anymore. From global logistics to the software architectures running smart cities, everything is interconnected. To handle this complexity, we need a shift in mindset. We have to move away from seeing a product as just a "collection of parts" and start seeing it as a dynamic system.</p>

              <h3>Defining Systems Thinking in My Own Words</h3>
              <p>To me, systems thinking is basically the art of seeing the "connective tissue" of a project. Instead of seeing a system as a static machine with a few separate parts, I view it as a dynamic web where everything affects everything else. It's a shift from asking, "What does this one part do?" to "How does this part influence the others, and how does the whole react to that change?"</p>
              <p>According to Jon Holt (2019), the core idea of systems theory is that parts are best understood in the context of their relationships with each other and with other systems, rather than in isolation. This is so important because systems exhibit "emergence." This is a concept that describes how a system as a whole can do things that the individual parts can't do on their own (Holt, 2019).</p>
              <p>Donella Meadows (2008) expands on this by defining a system as more than just a pile of things; it is an "interconnected set of elements that is coherently organized in a way that achieves something" (p. 11). If you ignore the connections, you aren't really seeing the system at all. You just have a list of parts without a purpose. I've found that in my own technical projects, like building risk models, focusing only on the code while ignoring how it interacts with the data inputs always leads to errors. Systems thinking forces you to look at that organization and purpose from the very beginning.</p>

              <h3>Breaking the Line: Systems vs. Linear Thinking</h3>
              <p>The biggest difference between systems thinking and traditional linear thinking is the difference between a straight line and a circle.</p>
              <ul>
                  <li><strong>Linear Thinking (Reductionism):</strong> This is the "cause-and-effect" approach. It assumes that if you break a problem down into small pieces and fix one piece at a time, you've solved the whole thing. It assumes you can predict how a system will act just by adding up the behaviors of its parts (Holt, 2019).</li>
                  <li><strong>Systems Thinking (Holism):</strong> This recognizes feedback loops. As Peter Senge (2006) explains, "systems thinking is a discipline for seeing wholes," which means we can actually identify the hidden patterns in a complex system instead of just focusing on small, isolated events that don't tell the whole story (p. 6).</li>
              </ul>
              <p>Senge (2006) also points out that in complex environments, cause and effect are often separated by long delays or happen in different areas. A change in one area might not show an impact until much later, or it might circle back and affect the original source in a "feedback loop". In our course reading, we see that "everything is connected to something else" (Holt, 2019). While a linear thinker might see a car as just an engine and a chassis, a systems thinker sees that car as a participant in a huge transport network—interacting with smart roads, satellites, and even environmental laws.</p>

              <h3>Categorizing the Systems We Build</h3>
              <p>To apply systems thinking effectively, we have to recognize that not all systems are built the same way. Peter Checkland developed a taxonomy that helps us organize these different types (Holt, 2019):</p>
              <ul>
                  <li><strong>1. Natural Systems:</strong> These are systems beyond our control, like weather or biological environments.</li>
                  <li><strong>2. Designed Physical Systems:</strong> These are the physical systems and hardware we create, like smartphones or spacecraft.</li>
                  <li><strong>3. Designed Abstract Systems:</strong> These are non-physical, like mathematical models, equations, or even thought experiments.</li>
                  <li><strong>4. Human Activity Systems:</strong> These consist of people interacting to achieve a goal, like a political group or a business organization.</li>
                  <li><strong>5. Transcendental Systems:</strong> These are systems that go beyond our current understanding.</li>
              </ul>
              <p>In the digital world, systems typically fit into multiple categories at once (Holt, 2019). Having completed my Google UX Design Professional Certificate, I've learned how vital the "Human Activity" piece is to a technical system. Systems thinking lets us bridge these categories to make sure the software concept (Abstract) actually works in the physical reality for the people (Human Activity) who need it. It's about realizing that a perfectly coded app is useless if the people using it don't understand the interface. My UX background has taught me that we have to design for the human element as part of the system, not as an afterthought.</p>

              <h3>The "Three Evils" of Engineering</h3>
              <p>The reason we need a systems-thinking approach is because it is very easy for things to go wrong. In real-world projects, unmanaged complexity can lead to total failure. Jon Holt (2019) identifies what he calls the "Three Evils of Systems Engineering"</p>
              <ul>
                  <li><strong>Complexity:</strong> This happens when we don't identify where the complexity is, which means we can't manage or control it. There are two types: Essential Complexity (natural to the system) and Accidental Complexity (caused by bad processes or tools).</li>
                  <li><strong>Communication:</strong> This is where information flow fails or becomes ambiguous. With so many different technical backgrounds on a project, miscommunication is a huge risk.</li>
                  <li><strong>Understanding:</strong> This is where different points of view (contexts) are ignored and people start making assumptions.</li>
              </ul>
              <p>These "evils" feed into one another. If you have unmanaged complexity, your communication will probably fail, which leads to a lack of understanding among the team (Holt, 2019). Systems thinking helps us troubleshoot these issues by forcing us to define boundaries and identify exactly who the stakeholders are and what they need.</p>

              <h3>Characteristics and Boundaries of a System</h3>
              <p>Every system has a structure made up of System Elements that interact (Holt, 2019). These interactions create Interfaces, which are the points where different parts of a system meet to exchange information or materials. If you don't define your interfaces correctly, your system will fail during integration. We also track Attributes, which are the properties like dimensions, weight, or data structures that define what the system "is" (Holt, 2019). Meadows (2008) notes that the most important part of a system is often its "function or purpose," and the attributes must all serve that goal.</p>
              <p>A practical part of systems thinking is defining the Boundary. The boundary explains the scope—what is inside the system and what is outside (Holt, 2019). In digital environments, this isn't always a physical wall; it can be conceptual, like the link between a car and a satellite. Inside the boundary, you have your system elements. Outside, you have your Stakeholders and Enabling Systems (Holt, 2019). Understanding these interfaces is crucial because failures usually happen at the points where different parts of a system meet.</p>

              <h3>Why MBSE is Applicable in Complex Digital Systems</h3>
              <p>As digital systems grow in complexity, traditional "document-based" engineering just doesn't cut it anymore. When you have knowledge scattered across Word documents or spreadsheets, it's nearly impossible to ensure that a change in one requirement doesn't break a design element buried in a different file. Model-Based Systems Engineering (MBSE) is a rigorous approach that moves the "single source of truth" from a pile of static documents to a dynamic digital model (Holt, 2019).</p>
              <ol>
                  <li><strong>1. Managing Complexity through Abstraction:</strong> A Model is a simplified representation of a system designed to capture the information needed to build it successfully. It doesn't need to show everything—just enough to get the job done (Holt, 2019). MBSE helps us focus on the essential parts while stripping away the noise. By using different "views," we can see exactly where the complexity lives.</li>
                  <li><strong>2. Bridging the Communication Gap:</strong> In any big project, you have software engineers, managers, and customers all speaking different "dialects." MBSE addresses this by establishing a Common Language. This includes a "Spoken Language" like SysML and a "Domain-Specific Language" called an Ontology (Holt, 2019). This ensures that when a stakeholder looks at a model, they see information tailored to their specific "context" while remaining consistent with the rest of the project.</li>
                  <li><strong>3. Rigorous Consistency and Behavior Modeling:</strong> In a digital system, the structure (what it is) and the behavior (how it acts) have to be perfectly aligned. MBSE ensures this consistency. If a sequence diagram shows a message being sent, the structural diagrams have to show an interface that allows it to happen (Holt, 2019). This rigor allows us to model "rainy day" scenarios—where we troubleshoot what happens when things go wrong—before we ever build a physical prototype.</li>
              </ol>

              <h3>The Mantra: People, Process, and Tools</h3>
              <p>Success in this field isn't just about the software; it's about the "Systems Engineering Mantra": People, Process, and Tools (Holt, 2019).</p>
              <ul>
                  <li><strong>People:</strong> It's about the competence of the team. They need the right skills and attitude to enable the approach.</li>
                  <li><strong>Process:</strong> This is the overall ability of the organization to carry out tasks effectively. It's the "what" and "how" of the work.</li>
                  <li><strong>Tools:</strong> This is the software and resources that help the people do the work.</li>
              </ul>
              <p>A huge systems-thinking insight here is that the process must drive the choice of tools, rather than letting the tools dictate how we work (Holt, 2019). Senge (2006) would likely describe this as a "mental model" shift—we have to change how we think about the work before the tools can have any real impact.</p>

              <h3>Needs, Requirements, and Constraints</h3>
              <p>Every system exists for a purpose, and we express that through needs. These can be broken down into Requirements (what it must do), Features (collections of functions), and Goals (high-level purpose) (Holt, 2019). However, we also have to deal with constraints. Constraints limit how we can build the system—they might be environmental, safety-related, or based on quality standards. Systems thinking helps us see how these constraints interact with our requirements. For example, a safety constraint might make it impossible to meet a specific speed requirement. Finding that out early in a model saves time and money (Holt, 2019).</p>

              <h3>The Evolution Toward Maturity</h3>
              <p>Moving from a document-based setup to a model-based one is a journey. Jon Holt (2019) describes five stages of evolution:</p>
              <ol>
                  <li><strong>Stage 1: Document-Based:</strong> Knowledge is scattered across independent files.</li>
                  <li><strong>Stage 2: Document-Centric:</strong> Some notations are used, but they are just "pictures" that don't own any data.</li>
                  <li><strong>Stage 3: Model-Enhanced:</strong> A true model starts to emerge and own some of the system knowledge.</li>
                  <li><strong>Stage 4: Model-Centric:</strong> The model is nearly complete and owns most of the knowledge.</li>
                  <li><strong>Stage 5: Model-Based:</strong> The model is a standalone entity that holds all the system data. Documents are just views generated from the model.</li>
              </ol>
              <p>Most organizations are somewhere in the middle of this growth process. It takes a growth mindset to move through these stages and realize that the model is the "single source of truth." By Stage 5, the model allows for automated checks and seamless integration between different tools.</p>

              <h3>Final Reflection</h3>
              <p>Embracing a model-based approach is about more than just using new tools; it's a shift in mindset. As we look toward 2035, the entire industry is heading toward a fully MBSE environment (Holt, 2019). By using systems thinking and my background in UX design, I'm learning to stop building isolated "parts" and start engineering successful, resilient systems that actually survive in our connected world.</p>
              <p>It's been a challenge to balance my classes and work lately, but I'm really motivated by these concepts because they show how a focused, technical approach can solve massive problems. I've always been a self-starter and a quick learner, and diving into the "Single Source of Truth" philosophy makes so much sense to me. I'm looking forward to applying these ideas as I dive deeper into IT 312 and my own projects!</p>

              <footer class="article-footer">
                  <h3>Sources:</h3>
                  <ol>
                      <li>Holt, J. (2019). <em>Systems Engineering Demystified</em>. Packt Publishing.</li>
                      <li>Meadows, D. (2008). <em>Thinking in Systems</em>. Earthscan. <a href="https://research.fit.edu/media/site-specific/researchfitedu/coast-climate-adaptation-library/climate-communications/psychology-amp-behavior/Meadows-2008.-Thinking-in-Systems.pdf">https://research.fit.edu/media/site-specific/researchfitedu/coast-climate-adaptation-library/climate-communications/psychology-amp-behavior/Meadows-2008.-Thinking-in-Systems.pdf</a></li>
                      <li>Senge, P. (2006). <em>The Fifth Discipline</em>. Doubleday. <a href="https://www.e-education.psu.edu/geog468/sites/www.e-education.psu.edu.geog468/files/TheFifth%20Discipline.pdf">https://www.e-education.psu.edu/geog468/sites/www.e-education.psu.edu.geog468/files/TheFifth%20Discipline.pdf</a></li>
                  </ol>
              </footer>
          </article>

          <!-- Module 1.2 Blog Post Starts here -->
          <article>
              <header class="article-header">
                  <h2>Module 1.2 Blog Post</h2>
              </header>
              <h3>Systems Development Life Cycle</h3>
              <p>Hi everyone! Im back for my second post in IT 312. After our deep dive into systems thinking last time, we're moving into something that feels practical for anyone building or managing digital environments: the Systems Development Life Cycle (SDLC). We've been reading through chapters three, four, and five of Systems Engineering Demystified (Holt, 2019), and it's honestly changed how I look at my own projects, like the cost and risk modeler I'm building on GitHub.</p>
              <p>When we think about a system, it's easy to just focus on the "cool" part—the actual building or the code. If we don't have a plan for how that system is going to be supported or retired, we're just creating a wasteful situation down the road.</p>

              <h3>Defining the Life Cycle</h3>
              <p>The Systems Development Life Cycle is essentially the life story of a system. It's the roadmap that takes an idea from a simple "What if?" all the way to the moment it's finally turned off and replaced. It isn't just a technical checklist; it's a way to manage the "Three Evils" we talked about last time—Complexity, Communication, and Understanding.</p>
              <p>According to Jon Holt (2019), a life cycle is a way to partition the life of a system into manageable chunks called stages. These stages give us a foundation to make sure we are actually meeting the needs of the stakeholders at every point in time. If you don't think in terms of a life cycle, you're basically just winging it, and that's how airplanes fall out of the sky or IT systems bring companies to their knees.</p>
              <p>The standard life cycle typically includes several key stages (Holt, 2019):</p>
              <ul>
                  <li><strong>Conception:</strong> Where the initial idea is born.</li>
                  <li><strong>Development:</strong> Where the system is actually designed and built.</li>
                  <li><strong>Production:</strong> Where we make copies or deploy the system for real-world use.</li>
                  <li><strong>Utilization:</strong> Where the system is actually doing its job.</li>
                  <li><strong>Support:</strong> Where we troubleshoot and maintain the system.</li>
                  <li><strong>Retirement:</strong> Where the system is phased out.</li>
              </ul>

              <h3>Systems and Interfaces</h3>
              <p>Before we dive into the models, we have to talk about how these systems are actually put together. Building on what I said in my last post about systems thinking being the “connective tissue” of a project, Chapter 3 takes that even further by looking at the actual physical and technical connective tissue: the interfaces. An interface is the boundary where two system elements meet to exchange information or materials.</p>
              <p>If we don't define these interfaces during the development stage, we end up with what I call the "Inedible System" problem. In a letter I helped my grandparents write to a chef, I mentioned that if vegetables in a soup are cut too large, they are impossible to eat with any dignity. Interfaces are the same. If they aren't defined into "bite-size" pieces that follow a clear protocol, the data can't "flow," and the system becomes a mess of gristly, unworkable parts. Thinking about the life cycle means designing these interfaces so they work not just today but five years from now during the utilization stage.</p>

              <h3>Comparing Life Cycle Models: Linear, Iterative, and Incremental</h3>
              <p>Not every project should follow the same path. Depending on what you're building, you might choose a different life cycle model. Holt (2019) breaks these down into three main types, and they each have their own pros and cons.</p>

              <h4>1. Linear Models</h4>
              <p>The linear model is the straight line of systems engineering. You do one stage, finish it completely, and then move to the next. It's very logical and easy to explain to people who aren't technical.</p>
              <ul>
                  <li><strong>Characteristics:</strong> Every stage has a clear start and end point. You don't move forward until the previous stage is perfect.</li>
                  <li><strong>Advantages:</strong> It's great for projects where the requirements are set in stone and aren't going to change. It feels very stable and organized.</li>
                  <li><strong>Potential Drawbacks:</strong> This is where things can get "inedible." If you wait until the very end to show the customer stakeholder the system, and it's not what they wanted, you've wasted a massive amount of time and money. According to the NASA Systems Engineering Handbook (2016), linear models are often criticized because they don't allow for the rework that usually happens when people realize they missed a requirement halfway through. If you can't backtrack easily, you're stuck with a "woody peach" of a system—it looks okay on the outside, but it's unusable.</li>
              </ul>

              <h4>2. Iterative Models</h4>
              <p>Iterative models are all about the growth mindset. Instead of trying to get it perfect in one go, you go through cycles. Each cycle (or iteration) gets you closer to the final goal.</p>
              <ul>
                  <li><strong>Characteristics:</strong> You repeat the development processes over and over, refining the system each time based on what you learned in the last cycle.</li>
                  <li><strong>Advantages:</strong> It's excellent for troubleshooting. You find mistakes early because you're constantly reviewing the model. It allows for a lot of flexibility if the requirements shift.</li>
                  <li><strong>Potential Drawbacks:</strong> It can feel like you're overthinking the project if you aren't careful. Without a clear process to drive the iterations, you might just keep going in circles without ever actually finishing. It requires a lot of discipline to know when a version is good enough for that cycle.</li>
              </ul>

              <h4>3. Incremental Models</h4>
              <p>Incremental models are about getting results fast. You break the system into chunks and deliver them one by one. I think this approach has advantages because it means the user gets to use part of the system while you're still building the rest.</p>
              <ul>
                  <li><strong>Characteristics:</strong> Each increment is a functional piece of the system. You build the foundation first, then add the features.</li>
                  <li><strong>Advantages:</strong> It reduces waste because the customer can see progress right away. As the Department of Defense Systems Engineering Guidebook (2024) explains, this incremental approach is the preferred method for modern digital systems because it allows us to provide capabilities to the user early and often while managing risks in smaller, more manageable pieces.</li>
                  <li><strong>Potential Drawbacks:</strong> The integration can get messy. If you don't have a solid architecture or ontology from the start, the different increments might not talk to each other correctly when you try to integrate them at the end.</li>
              </ul>

              <h3>Deep Dive: The V-Model and Technical Management</h3>
              <p>One concept from the reading that really stuck with me is the V-model. It's a way to visualize how we move from high-level requirements down to detailed design and then back up through testing and verification (Holt, 2019). The left side of the V is where we define what we want, and the right side is where we check to make sure we actually built it right.</p>
              <p>This is where the Technical Management Processes from Chapter 5 come in. To keep a project from spiraling out of control, we have to use processes like:</p>
              <ul>
                  <li><strong>Risk Management:</strong> Identifying potential problems early.</li>
                  <li><strong>Configuration Management:</strong> Making sure we know exactly which version of the model we are working on so we don't have a communication failure.</li>
                  <li><strong>Information Management:</strong> Ensuring that the right people have the right data at the right time.</li>
                  <li><strong>Agreement Processes:</strong> These are the supply and acquisition processes (Holt, 2019). They make sure that when we buy a component or a service, we actually get what we paid for.</li>
              </ul>
              <p>If we don't have these processes in place, our life cycle is just a series of accidents. I've realized that failing to document a technical issue properly means the next person has to repeat the entire troubleshooting process from scratch. That is a total waste of time and energy (Holt, 2019).</p>

              <h3>Why Life Cycle Thinking is Critical for Digital Systems</h3>
              <p>Reflecting on all this, I've realized that thinking in terms of a life cycle is the only way to ensure sustainability and evolution. In IT, we often get caught up in the deployment phase, but we forget about the retirement or support phases.</p>

              <h4>1. Design and Sustainability</h4>
              <p>If we don't plan for the support stage during the development stage, the system is going to be a nightmare to maintain. This is exactly what I mean when I talk about attention to detail. Digital systems are the same way. If you don't cut your code or your hardware interfaces correctly during the design phase, the support team is going to struggle to fix it later. Sustainability means the system can keep running without being a drain on resources.</p>

              <h4>2. Evolution and the Single Source of Truth</h4>
              <p>Digital systems are never truly finished. They have to evolve. By using Model-Based Systems Engineering (MBSE) within our life cycle, our model becomes the single source of truth that allows us to see how a change in 2026 might impact the system in 2030 (Holt, 2019). This helps us avoid accidental complexity that comes from patching old systems without understanding the original design.</p>

              <h4>3. Retirement: Avoiding the Wasteful End</h4>
              <p>One of my biggest pet peeves is waste. In the system life cycle, waste happens when we don't plan for the end of a process. The retirement stage is where we figure out how to transition data and hardware responsibly. If we don't plan for retirement, we end up with zombie systems that are insecure and just take up space and power without providing any real value.</p>

              <h3>My Reflection: The People, Process, Tools Mantra</h3>
              <p>Success in IT 312 isn't just about knowing the definitions; it's about seeing how they all link together. I've learned that whether I'm developing a complex satellite model or tackling a new technical challenge, I have to follow a process. The life cycle models give us that process.</p>
              <p>I'm a self-starter, and I love a challenge, but I also know that I have to balance my credit load with my work hours. Thinking in terms of a life cycle for my own time management has helped me stop leaving things to the last day. I'm motivated to keep closing my knowledge gaps in IT and cybersecurity, and I think understanding these life cycle models is going to be a huge part of my growth.</p>

              <footer class="article-footer">
                  <h3>References:</h3>
                  <ol>
                      <li>Hirshorn, S. (2007). <em>NASA Systems Engineering Handbook</em>. <a href="https://www.nasa.gov/wp-content/uploads/2018/09/nasa_systems_engineering_handbook_0.pdf">https://www.nasa.gov/wp-content/uploads/2018/09/nasa_systems_engineering_handbook_0.pdf</a></li>
                      <li>Holt, J. (2019). <em>Systems engineering demystified</em> (2nd ed.). Packt Publishing.</li>
                      <li>Office of the Under Secretary of Defense for Research and Engineering. (2022). <em>Systems Engineering Guidebook</em>. <a href="https://www.dau.edu/sites/default/files/Migrated/CopDocuments/Systems-Eng-Guidebook%20Feb2022-Cleared.pdf">https://www.dau.edu/sites/default/files/Migrated/CopDocuments/Systems-Eng-Guidebook%20Feb2022-Cleared.pdf</a></li>
                  </ol>
              </footer>
          </article>

          <!-- Module 2.1 Blog Post Starts here -->

          <article>
              <header class="article-header">
                  <h2>Module 2.1 Blog Post</h2>
              </header>
              <h3>Designing for People: Beyond Pixels and Code</h3>
              <p>Hello, everyone! If you’ve read my last couple of posts, you know we’ve been exploring systems thinking and the structured phases of the System Development Life Cycle. Those frameworks are incredibly helpful for understanding how technology fits together, but they can also feel a little… clinical. Lately, I’ve been thinking more about the human side of all of this. It’s one thing to build a system that technically works, but it’s another thing entirely to build something that respects the person on the other side of the screen.</p>
              <p>For me, designing for people means moving past treating data as numbers on a spreadsheet and actually practicing the empathy needed to understand how our brains process information in the first place. It’s about remembering that behind every interface is a real person with their own frustrations, habits, fears, and expectations. And if we ignore that, even the most technically impressive system can fall flat.</p>

              <h3>The Heart of Human‑Centered Design</h3>
              <p>Human‑Centered Design (HCD) is all about putting real people at the center of the development process. Instead of asking, “How do we build this?” HCD asks, “Who are we building this for, and why does it matter to them?”</p>
              <p>In my earlier posts, I focused more on the structural side of systems. HCD flips that around. It reminds us that a system isn’t successful just because it functions—it’s successful when it genuinely helps someone and fits naturally into their life.</p>
              <p>Professionals often look at two major perspectives when talking about HCD: the ISO 9241‑210 standard and IDEO’s Field Guide.</p>
              <ul>
                  <li><strong>ISO 9241-210</strong></li>This gives a very structured, ergonomic-focused definition of HCD. It emphasizes effectiveness, efficiency, and satisfaction. It’s the "technical backbone" that ensures we're building something usable and iterating based on real feedback.</li>
                  <li><strong>IDEO’s Field Guide</strong></li>takes a more hands-on, people-first approach. It focuses on empathy, curiosity, and the belief that design can actually improve someone’s day-to-day life (IDEO.org, 2015).</Li>
              </ul>
              <p>IDEO breaks this mindset into three phases:</p>
              <ul>
                  <li><strong>Inspiration:</strong> Immerse yourself in people's lives and understand their needs.</li>
                  <li><strong>Ideation:</strong> Make sense of what you learned and start prototyping.</li>
                  <li><strong>Implementation:</strong> Bring the solution to life in a way that works in the real world.</li>
              </ul>
              <p>Both perspectives matter. ISO helps you build something right. IDEO helps you build the right thing.</p>
              <p>One thing I appreciate about HCD is that it doesn’t assume the designer knows best. It assumes the user knows best. And honestly, that mindset alone can prevent so many problems. I’ve seen systems that were clearly built by people who never once talked to the people who would actually use them. You can feel it immediately—everything takes too many steps, nothing is where you expect it to be, and you end up fighting the system instead of working with it.</p>
              <p>I’ve also noticed that when designers skip the “human” part of the process, they often end up patching problems later that could have been avoided entirely. It’s like building a house and only afterward realizing you forgot to put in any electrical outlets. You can fix it, but it’s going to be messy, expensive, and frustrating for everyone involved.</p>

              <h3>Perception, Attention, and the Visual Hierarchy</h3>
              <p>Something I’ve noticed—especially when working with messy data—is that information can look like complete gibberish if it isn’t organized well. Our brains are wired to look for patterns, and when a design doesn’t follow a clear visual hierarchy, everything feels harder than it needs to be.</p>
              <p>Good design quietly guides your eyes using size, color, spacing, and placement. You shouldn’t have to work to figure out what matters.</p>
              <p>Our perception isn’t just about seeing; it’s about interpreting. We’re biologically tuned to notice edges, contrast, and structure. That’s why a brightly colored “Apply” button stands out before we even read it. Our brains prioritize high‑contrast elements because they help us navigate the world quickly. When designers ignore how people naturally perceive information, they create unnecessary cognitive load—basically, they make us think harder than we should.</p>
              <p>For example:</p>
              <ul>
                  <li>When items are close together, our brain groups them automatically (the Proximity Principle).</li>
                  <li>When shapes or colors match, we assume they share a function (Similarity).</li>
                  <li>When elements follow a smooth path, our eyes follow it too (Continuity).</li>
              </ul>
              <p>The best designs feel intuitive because they work with our biology, not against it (Interaction Design Foundation, 2016; Johnson, 2010).</p>
              <p>I’ve experienced this firsthand when helping new managers at Amazon navigate Excel. When data is spaced poorly or labels don’t align with their fields, people get confused—even if the information is technically correct. But once you clean up the spacing, group related items, and highlight the important parts, suddenly everything “clicks.” It’s a reminder that clarity isn’t just a design preference; it’s a cognitive necessity.</p>
              <p>Another example is my own experience learning data analytics. When I first started my mentorship, the dashboards and spreadsheets looked overwhelming. But as I learned more about how visual hierarchy works, I started to understand why some dashboards felt intuitive and others felt like a maze. It wasn’t the data—it was the design.</p>

              <h3>The Laws of UX: Designing for the Mind</h3>
              <p>If we want to design for people, we have to understand the psychological principles behind how we interact with digital spaces. A few of the big ones (Yablonski, 2024):</p>
              <h4>Jakob’s Law</h4>
              <p>People spend most of their time on other sites, so they expect yours to work the same way. This is why moving a shopping cart icon to the bottom left “just to be different” usually backfires.</p>
              <h4>Fitts’s Law</h4>
              <p>The time it takes to reach a target depends on its size and distance. This is why important buttons on mobile apps are big and placed where your thumb naturally rests.</p>
              <h4>Hick’s Law</h4>
              <p>More choices = slower decisions. If a home screen has twenty options, people freeze. Chunking information or offering a few clear paths helps reduce that overwhelm.</p>
              <h4>Miller’s Law</h4>
              <p>People can only hold about seven items in working memory. Designs that expect users to remember too much from one screen to the next end up frustrating them.</p>
              <p>What I love about these laws is that they’re not just design rules—they’re reflections of how our brains actually work. They remind us that people aren’t machines. We get overwhelmed. We get distracted. We forget things. And that’s normal (Budiu, 2024).</p>
              <p>These laws also explain why some apps feel effortless and others feel like a chore. When a design aligns with how our minds naturally operate, we barely notice the interface at all. We just do what we need to do. But when a design ignores these principles, we feel it immediately—usually as frustration, confusion, or the urge to close the app altogether.</p>

              <h3>The Responsibility of the Designer</h3>
              <p>I genuinely believe empathy is a technical skill. As technology becomes more automated and starts making decisions for us, critical thinking becomes even more important. Designers have to consider privacy, data use, and the emotional impact of their choices. One of the biggest challenges today is the rise of systems that make decisions without explaining why. Transparency gives users back a sense of control. When people understand how something works—and what data it uses—they feel safer and more respected.</p>
              <p>Designers also need to avoid “dark patterns,” which trick users into doing things they didn’t intend. Human‑centered design rejects that entirely. Respecting someone’s attention and autonomy builds trust, and trust is far more valuable than a quick conversion.</p>
              <p>Another responsibility designers carry is accessibility. It’s easy to design for people who think and navigate the world the same way we do. It’s much harder—and much more important—to design for people who don’t. That includes people with disabilities, people who speak different languages, people who are stressed or tired, and people who are simply unfamiliar with technology. When we design with empathy, we acknowledge that not everyone interacts with systems the same way.</p>
              <p>I also think designers have a responsibility to consider the emotional experience of the user. Technology can either empower people or make them feel small. A confusing interface can make someone feel incompetent. A thoughtful one can make them feel capable. That emotional impact matters more than we often admit.</p>

              <h3>Bridging the Gap Between Technical and Human</h3>
              <p>Designing for people means connecting the technical systems I’ve talked about in earlier posts with the real experiences of the people who use them. Whether it’s a massive network or a simple interface, the goal is the same: present information honestly, clearly, and in a way that supports the user.</p>
              <p>Staying open-minded and willing to consider different perspectives is essential. When we combine the structure of ISO with the empathy of IDEO, we create technology that doesn’t just function—it actually helps people. For me, that’s what being driven in this field looks like: building something that doesn’t just run on a server but genuinely works for the person sitting in front of the screen.</p>
              <p>In the end, systems thinking and HCD complement each other. Systems thinking helps us understand complexity. HCD ensures that complexity serves a human purpose. Without both, we risk building efficient systems that confuse or even harm the people they were meant to support.</p>
              <p>And honestly, that’s the part of technology that excites me the most. It’s not just the code or the architecture—it’s the possibility of creating something that genuinely improves someone’s life, even in a small way. That’s the kind of design that lasts.</p>

              <h3>Final Reflection</h3>
              <p>Designing for people isn't a one-time task; it's a constant process of listening, observing, and refining. It requires us to look past our own biases and "overthinking" to see how others interact with what we've built. As I move forward with my own projects and learning, I want to make sure I’m always advocating for the user. Technology should empower us to interact with our environment more easily, not make our lives more complicated.</p>
              <p>By following these UX laws and staying focused on human-centered principles, we can build a digital future that is as intuitive as it is powerful. I’m excited to keep exploring these concepts and finding ways to make the complex world of technology a little more human.</p>

              <footer class="article-footer">
                  <h3>References:</h3>
                  <ol>
                      <li>Budiu, R. (2024). <em>Memory recognition and recall in user interfaces</em>. Nielsen Norman Group.</li>
                      <li>IDEO.org. (2015). <em>The Field Guide to Human-Centered Design</em>.</li>
                      <li>Interaction Design Foundation. (2016). <em>What Are Gestalt Principles?</em></li>
                      <li>Johnson, J. (2010). <em>Designing with the mind in mind</em>. Morgan Kaufmann.</li>
                      <li>Yablonski, J. (2025). <em>Laws of UX</em>.</li>
                  </ol>
              </footer>
          </article>

              <!-- Module 2.2 Blog Post Starts here -->
               <article>
                    <header class="article header">
                        <h2>Module 2.2 Blog Post</h2>
                    </header>
                    <h3>Memory, Habits, and the Mental Models We Carry</h3>
                    <p>Building a digital environment that actually works for people is a lot harder than it looks. In my recent studies of human-computer interaction, I've realized that usability isn't just about a pretty interface; it's about how that interface respects the way our brains are wired. When a system is truly usable, it feels like it's working with you, not against your memory, habits, and ingrained expectations.</p>
                    <p>One of the biggest takeaways from Jeff Johnson's Designing with the Mind in Mind is that our memory is incredibly imperfect (Johnson, 2010). We don't store pixel-perfect images of every screen we see; instead, we rely on "mental models," which are basically our internal shortcuts for how we think a system should work. These models are formed through years of interacting with the world and other digital products (Olaverri-Monreal & Goncalves, 2014).</p>
                    <p>This is exactly what Jakob's Law describes: users spend most of their time on other sites, so they expect your site to work just like those do (Yablonski, 2020). When a designer ignores these established patterns, they force the user to learn a new model from scratch, which takes up precious cognitive energy and often leads to frustration. In my own experience building a video game in Unreal Engine 5, I have to be careful not to reinvent the wheel for basic controls. If the pause menu isn't where players expect it to be, it breaks their immersion because it clashes with their mental model of how games function.</p>
                    <h4>The Limits of Working Memory</h4>
                    <p>Our working memory is especially fragile, capable of holding only a few items at a time. Digital systems that require us to remember information from one screen to the next are essentially setting us up for failure. As Johnson explains, we are much better at recognition than we are at recall (Johnson, 2010). This is why a well-designed dashboard is so effective—it puts all the necessary data right in front of you so you don't have to fish through your memory to remember a specific figure from a previous page.</p>

                    <h3>The Power of Consistency and Feedback</h3>
                    <p>To make a system intuitive, you have to lean into consistency. Johnson points out that our perception is heavily biased by our goals and past experiences (Johnson, 2010). If a system stays consistent, we can rely on our habits rather than our active, effortful thinking.</p>
                    <p>Consistency isn't just about looks; it's about behavior. If a "submit" button is blue on one page but red on another, it creates a "speed bump" for the brain. I use GitHub to organize my projects because it's professional and easy to navigate. The consistent layout across repositories means I don't have to hunt for the "Settings" tab every time I start a new project—it's always where I expect it to be based on my previous interactions.</p>
                    <h4>System Feedback: Closing the Loop</h4>
                    <p>System feedback is just as crucial for reducing uncertainty. When we interact with a device, we need to know that our action was registered. Think about a Norman Door—those frustrating doors that have a handle you want to pull, but you actually have to push (Johnson, 2010). Digital systems do this too. If I click a button and nothing happens for three seconds, I start overthinking. Good design provides immediate feedback, like a loading spinner or a button changing color, to tell the brain, "I heard you; I'm working on it." (Yablonski, 2020).</p>
                    <figure style="margin: 20px 0;">
                        <img src="C:\CWU Winter Quarter 2026\Designing Digital Environments\cwu-itm.github.io\degregoryn\Images\kelsey-dody-R5ybRdRevi0-unsplash.jpg" alt="Norman door. A glass door with handle." style="max-width: 100%; height: auto; border-radius: 4px;">

                        <figcaption style="margin-top: 10px; line-height: 1.5;">
                            Figure 1<br>
                            <a href="https://unsplash.com/photos/reflections-of-trees-and-grass-in-glass-doors-R5ybRdRevi0?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText" target="_blank">"Gateway"</a>
                            by <a href="https://unsplash.com/@_kelseycam?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText" target="_blank">Kelsey Dody</a>
                            via <a href="https://unsplash.com/" target="_blank">Unsplash</a>,
                            used under the <a href="https://unsplash.com/license" target="_blank">Unsplash License</a>.
                        </figcaption>
                    </figure>
                    
                    <h3>Tesler's Law: The Conservation of Complexity</h3>
                    <p>When we talk about making things easier, we have to mention Tesler's Law, also known as the Law of Conservation of Complexity. It states that for any system, there's a certain amount of complexity that just can't be removed. The real question is who has to deal with it—the designer or the user? (Yablonski, 2020).</p>
                    <p>In my cybersecurity studies, I see this with multi-factor authentication (MFA). It's a complex process to keep data secure, but if the designers make the MFA process too clunky, the burden falls on the user, and they might just find a less secure workaround. As Larry Tesler argued, engineers should spend an extra week making the software smarter so that a million users don't have to waste a minute every day dealing with that complexity (Yablonski, 2020).</p>
                    <figure style="margin: 20px 0;">
                        <img src="c:\CWU Winter Quarter 2026\Designing Digital Environments\cwu-itm.github.io\degregoryn\Images\beatriz-cattel-ajTN690FnUg-unsplash.jpg" alt="A hand drawing a organizational diagram on a glass whiteboard with a black marker." style="max-width: 100%; height: auto; border-radius: 4px;">

                        <figcaption style="margin-top: 10px; line-height: 1.5;">
                            Figure 2<br>
                            <a href="https://unsplash.com/photos/hand-drawing-a-diagram-on-a-whiteboard-ajTN690FnUg?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText" target="_blank">"Hand drawing a diagram on a whiteboard"</a>
                            by <a href="https://unsplash.com/@bicattel?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText" target="_blank">Beatriz Cattel</a>
                            via <a href="https://unsplash.com/" target="_blank">Unsplash</a>,
                            used under the <a href="https://unsplash.com/license" target="_blank">Unsplash License</a>.
                        </figcaption>
                    </figure>
                    
                    <h3>Habit Formation and the Path of Least Resistance</h3>
                    <p>Building a habit is the brain's way of automating a task to save energy. According to Johnson, once a behavior becomes a habit, it moves from our conscious working memory into a more permanent, automatic part of the brain (Andrews et al., 2015). This is why consistency is the holy grail of usability. If the "Save" button is always in the same place, I stop looking for it and start knowing where it is.</p>
                    <p>However, this also means that when a system changes its layout—even if the new layout is technically better—it causes immediate distress. This is known as the "negative transfer" of learning. I felt this recently when one of the software tools I use for class updated its interface. Even though the new version was sleeker, my muscle memory kept clicking where the old buttons used to be. It took twice as long to finish my work because I had to fight my own habits.</p>
                    <h4>Fitts's Law and Physical Interaction</h4>
                    <p>Another relevant principle is Fitts's Law, which states that the time to acquire a target is a function of the distance to and size of the target. In digital design, this means that important buttons should be large and placed in areas that are easy to reach. If a "Delete Everything" button is tiny and placed right next to the "Save" button, it's a disaster waiting to happen. Good usability isn't just about thinking; it's about the physical ease of interaction (Yablonski, 2020).</p>
                    
                    <h3>Cognitive Load and Decision Paralysis</h3>
                    <p>One of the most common ways usability breaks down is through Hick's Law, which says that the time it takes to make a decision increases with the number and complexity of choices. We see this everywhere in modern tech. When a website gives you twenty different tabs and fifty different buttons on the home screen, your brain essentially freezes. This is what we call analysis paralysis (Yablonski, 2020).</p>
                    <p>In one of my data analysis classes, I learned how important it is to simplify information. If a data visualization has too many colors, labels, and lines, the viewer can't see through it to find the pattern. The same applies to interface design. By narrowing the choices available to the user at any given moment, designers can guide the user through a process without overwhelming their cognitive capacity (Johnson, 2010; Yablonski, 2020).</p>
                    <figure style="margin: 20px 0;">
                        <img src="c:\CWU Winter Quarter 2026\Designing Digital Environments\cwu-itm.github.io\degregoryn\Images\luke-chesser-JKUTrJ4vK00-unsplash.jpg" alt="A laptop screen displaying multiple colorful performance analytics graphs and data dashboards." style="max-width: 100%; height: auto; border-radius: 4px;">

                        <figcaption style="margin-top: 10px; line-height: 1.5;">
                            Figure 3<br>
                            <a href="https://unsplash.com/photos/graphs-of-performance-analytics-on-a-laptop-screen-JKUTrJ4vK00?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText" target="_blank">"Graphs of performance analytics on a laptop screen"</a>
                            by <a href="https://unsplash.com/@lukechesser?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText" target="_blank">Luke Chesser</a>
                            via <a href="https://unsplash.com/" target="_blank">Unsplash</a>,
                            used under the <a href="https://unsplash.com/license" target="_blank">Unsplash License</a>.
                        </figcaption>
                    </figure>
                    <h4>The Aesthetics-Usability Effect</h4>
                    <p>It's also interesting to note the Aesthetics-Usability Effect, which suggests that users often perceive more aesthetically pleasing designs as being more usable. This is a bit of a trap! Just because a site looks like a modern work of art doesn't mean it actually functions well. I've visited several travel sites for Salem and Boston that look incredible, but they are so hard to navigate that I end up leaving before I can book anything. True usability happens when the beauty of the design supports—rather than distracts from—the function (Yablonski, 2020).</p>

                    <h3>The Vital Role of Information Architecture</h3>
                    <p>Beyond the individual buttons and colors, we have to consider the overall structure of information, or Information Architecture. Johnson argues that how we categorize information should mirror how users naturally think about it, not how the system is technically built. If I'm looking for a specific cybersecurity policy in a company database, I shouldn't have to know the server's file structure to find it. It should be categorized by "Security," "Policies," or "Employee Conduct." (Johnson, 2010).</p>
                    <p>When Information Architecture is poor, users feel "lost in hyperspace." They click through five different menus only to realize they are in the wrong section entirely. This is why breadcrumbs and clear navigation headers are so essential. They provide a map for the user's mental model, letting them know exactly where they are and how to get back to the home screen. A well-organized system reduces the fear of exploration, making the user feel more in control (Johnson, 2010).</p>

                    <h3>Reflection</h3>
                    <p>Poor design creates unnecessary cognitive load, which is basically just mental clutter that slows us down. I had a personal experience with this recently when I tried to take a "gamified" quiz for class. The interface was so confusing that I ended up answering 270 questions three times in a row, only for the score to not even record. That's eight hours of my life I won't get back because the system's "game" mechanics didn't align with the actual goal of recording a grade. It was a perfect example of design failing to respect the user's time and existing mental model of how a quiz should work (Yablonski, 2020).</p>
                    <h4>Usability as an Ethical Choice</h4>
                    <p>As I move toward a career in cybersecurity and AI, I'm realizing that usability is also an accessibility and ethical issue. If a system is so complex that only a few people can figure out how to use it, it creates a digital divide between the haves and have-nots. Furthermore, "Availability" in the CIA Triad means that data needs to be accessible when it's needed. If a security system is so unintuitive that it prevents authorized users from doing their jobs—like a nurse not being able to access patient data because the login screen is too complex—it has failed its primary mission.</p>
                    <h4>Predictive Behavior and the Future of UX</h4>
                    <p>As we integrate more AI into our daily lives, the way we interact with technology is shifting from typing and tapping to voices and gestures. We are moving toward a world of anticipatory design, where systems try to predict what we need before we even ask for it. While this can reduce cognitive load, it also raises huge privacy concerns. If my smart TV knows I'm likely to watch a certain show, that's convenient. But if a system starts making decisions for me without my input, it threatens my sense of autonomy.</p>
                    <p>By understanding the laws of UX and the biology of the mind, we can build tools that feel like natural extensions of our own capabilities rather than obstacles we have to overcome. We need to prioritize systems that respect our limited memory, leverage our existing habits, and provide clear, honest feedback.</p>

                    <footer class="article-footer">
                        <h3>References</h3>
                        <ol>
                            <li>Andrews, S., Ellis, D. A., Shaw, H., & Piwek, L. (2015). Beyond Self-Report: Tools to Compare Estimated and Real-World Smartphone Use. <em>PLOS ONE</em>, 10(10), e0139004. https://doi.org/10.1371/journal.pone.0139004</li>
                            <li>Johnson, J. (2010). <em>Designing with the mind in mind: Simple guide to understanding user interface design guidelines</em>. Morgan Kaufmann.</li>
                            <li>Olaverri-Monreal, C., & Gonçalves, J. (2014). Capturing mental models to meet users expectations. <em>2014 9th Iberian Conference on Information Systems and Technologies (CISTI)</em>, 1-5. https://doi.org/10.1109/cisti.2014.6877006</li>
                            <li>Yablonski, J. (2020). <em>Laws of UX: Using psychology to design better products & services</em>. O'Reilly Media, Incorporated.</li>
                        </ol>
                    </footer>




               </article>


              <!-- Module 2.3 Blog Post Starts here -->
              <!-- Module 3.1 Blog Post Starts here -->
              <!-- Module 3.2 Blog Post Starts here -->

      </main>
<!-- Page Footer  -->
      <footer class="page-footer">
       <p>Copyright &copy; 2025</p>
      </footer>
   </body>
</html>