<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <link href="css/styles.css" rel="stylesheet">
      <title> Ramiro Casas | Portfolio</title>
   </head>
   <body>
<!-- Page Header  -->
      <header class="page-header">
         <h1>Portfolio</h1>
      </header>
<!-- Main Content Area  -->
      <main>
         <!-- Module 1.1 Blog Post Starts Here-->
         <article>
            <header class="article-header">
               <h2>Module 1.1 Blog Post</h2>
            </header>
            <h3>Systems Thinking, Linear Thinking, and Why Models Matter in Complex Systems</h3>

<p>
When people talk about systems thinking, it often gets framed as something abstract or overly theoretical, but in practice it is much more concrete than that. Systems thinking, in my own words, is the ability to see the entire lifecycle of something from start to finish and understand how every decision, detail, and interaction contributes to the final outcome. It is cradle-to-grave thinking. That means not just focusing on what a system does, but how it is built, who it is built for, how people will actually use it, and how today’s choices create consequences later that circle back in ways that are easy to miss if you are only focused on the task in front of you.
</p>

<p>
At its core, systems thinking is about refusing to oversimplify. It is paying attention to the small details that usually get brushed off as someone else’s problem. Those details include timelines, resources, communication, human behavior, constraints, and even the language used to explain how something works. A systems thinker understands that systems do not exist in isolation. They interact with people, environments, and other systems, and those interactions matter just as much as the technical components themselves. As Holt explains, system behavior emerges from interactions rather than individual parts acting alone (Holt, 2019).
</p>

<p>
This approach differs significantly from more traditional, linear ways of thinking. Linear thinking assumes that problems can be broken down into separate pieces, solved one at a time, and then reassembled into a working whole. Cause and effect are treated as direct and predictable. If something goes wrong, the instinct is to identify a single point of failure and fix it. While this approach can work for simple problems, it becomes unreliable as systems grow more complex.
</p>

<p>
The issue with linear thinking is not that it is completely wrong, but that it is incomplete. It encourages short-term fixes and local optimization without fully considering downstream effects. In real-world projects, especially in construction and digital systems, this leads to repeated delays, rework, and unexpected failures. Decisions made early without full context often return later as larger problems. Linear thinking treats these outcomes as isolated incidents instead of recognizing them as predictable consequences of earlier choices.
</p>

<p>
Systems thinking challenges this mindset by focusing on feedback loops and long-term behavior. Instead of asking what broke, it asks why the system was structured in a way that allowed the failure to happen. It recognizes that actions do not move forward in a straight line. They loop back. Human behavior plays a critical role in this process. People misunderstand instructions, adapt to poor designs, work around constraints, and respond to incentives in ways that plans rarely account for. Ignoring these realities is one of the most common reasons systems fail repeatedly.
</p>

<p>
This distinction becomes especially important when designing complex digital systems. Digital systems are dynamic. They evolve over time, scale under changing conditions, and interact continuously with users and other systems. Attempting to design these systems using purely linear reasoning leads to fragile solutions that break as soon as assumptions no longer hold. Von Bertalanffy’s general systems theory emphasizes that complex systems cannot be understood solely by analyzing their components in isolation, because relationships fundamentally shape system behavior (von Bertalanffy, 1968).
</p>

<p>
A model-based approach aligns naturally with systems thinking because it forces designers to engage with those relationships early and explicitly. Model-Based Systems Engineering replaces disconnected documentation with shared models that represent system structure and behavior. These models make assumptions visible and highlight dependencies, constraints, and interfaces that might otherwise be overlooked. Holt notes that models allow teams to reason about complexity before it becomes unmanageable rather than reacting after failures occur (Holt, 2019).
</p>

<p>
In the context of complex digital systems, this matters because intuition alone is not sufficient. There are too many interactions happening at once, and many effects are delayed or indirect. Models allow designers to test scenarios, evaluate trade-offs, and understand how changes ripple through the system without risking the live environment. They also improve communication by providing a shared reference point, reducing ambiguity and guesswork across teams.
</p>

<p>
Organizations such as the International Council on Systems Engineering emphasize that model-based approaches are essential as systems become increasingly interconnected and adaptive. Models act as a single source of truth that evolves alongside the system, making it easier to manage change and reduce unintended consequences over time (INCOSE, 2022).
</p>

<p>
Ultimately, systems thinking is about responsibility. It requires doing the work early, thinking through consequences, and acknowledging that shortcuts taken in one area often surface later as larger problems elsewhere. Linear thinking may appear efficient in the moment, but systems thinking is what allows complex digital systems to function reliably in the real world. A model-based approach gives this way of thinking structure, making complexity manageable rather than overwhelming.
</p>

<footer class="article-footer">
<p>
Holt, J. (2019). <em>Systems engineering demystified</em> (2nd ed.). Packt Publishing.
</p>
<p>
von Bertalanffy, L. (1968). <em>General system theory: Foundations, development, applications</em>. George Braziller.
</p>
<p>
International Council on Systems Engineering. (2022). <em>INCOSE vision 2035</em>. https://www.incose.org
</p>
</footer>

         </article>

         <!-- Module 1.2 Blog Post Starts here -->
<body>

<article>
    <h1>Life Cycle Models: Human Frameworks That Shape More Than Just Projects</h1>

    <p>Life cycle models are the invisible architecture of project work. They guide not just technical tasks but the actual human experience of being on a project. They quietly dictate the pace, the atmosphere, and the nature of collaboration within a team. When a systems engineer picks a life cycle model, it isn’t simply a technical or procedural choice. It is a decision that shapes communication patterns and determines when and how errors surface. These models become the social scaffolding of a project. They influence how people relate to each other, how much uncertainty they can tolerate, and which behaviors are rewarded or punished (Holt & Weilkiens, 2023).</p>

    <p>A systems engineer’s job stretches far beyond managing schedules, budgets, and specs. They are stewards of human dynamics over long timelines. Teams may work together for years, and the life cycle model they adopt will set the tone for how that journey is experienced. It affects how people cope with setbacks, where they direct their anxieties, and how they respond to the unknown. Holt and Weilkiens (2023) describe the three main models that help manage this complexity: linear, iterative, and incremental. Each serves a different human purpose, balancing predictability, learning, and delivery of value.</p>

    <h2>The Linear Model and the Human Need for Predictability</h2>

    <p>The traditional linear model caters to a deep human need for order. It offers a reassuring sense of progress where each phase is finished before the next begins. This structure is comforting in high-stakes environments where uncertainty can trigger anxiety. People can focus on their roles, confident there is a master plan guiding the whole (Holt & Weilkiens, 2023).</p>

    <p>But beneath this sense of control lies a real human risk. Early decisions are locked in, rarely revisited, making it difficult to adapt when new information arises. When problems appear late, blame often falls on the people who made the initial assumptions, rather than acknowledging the unpredictability of complex systems. The culture can become hostile to change, which encourages risk aversion and hiding mistakes. For some teams, linear offers stability; for others, it creates fear around adaptation.</p>

    <p>The social cost shows up in the silos it builds. Communication becomes transactional rather than collaborative. People focus narrowly on their phase and overlook the bigger system. When the project hits a wall, fingers point backward, and trust erodes. That erosion can harm an organization more than any technical failure.</p>

    <h2>The Iterative Model and the Psychology of Learning</h2>

    <p>By contrast, the iterative model accepts that no one knows everything at the start. It assumes systems—and the people building them—evolve through cycles of trial and error. It normalizes uncertainty by making it clear that assumptions will be tested and revised. Teams are encouraged to admit gaps and to learn from both wins and failures. Culture shifts from judging people for mistakes to celebrating those who help the group eventually succeed (Edmondson, 1999).</p>

    <p>This creates psychological safety. Edmondson (1999) defines it as the shared belief that a team is safe for interpersonal risk taking. In iterative environments, a failed test is data, not disaster. This enables real innovation, because people feel safe to propose creative or unconventional ideas. Iteration does demand emotional resilience. Progress can feel uneven, and the lack of clear endpoints frustrates those who crave closure. Leaders must maintain momentum even when the path forward is unclear.</p>

    <h2>The Incremental Model and the Management of Value</h2>

    <p>Incremental focuses on delivering tangible results in stages, which builds trust through visible progress. Stakeholders see functioning pieces early and often, reducing anxiety and making feedback integral rather than late-stage. Users become co-creators, shaping the system as it develops (Alami & Ernst, 2024).</p>

    <p>Breaking work into chunks makes failure less catastrophic. Mistakes are smaller and easier to understand. Teams feel ownership, reinforcing that the system serves people, not the other way around. This approach is particularly useful when user needs are evolving. Frequent wins keep morale high. Instead of waiting years for results, the team gets small victories every few months.</p>

    <h2>A Reflection on the Academic Life Cycle</h2>

    <p>Sitting here in my final quarter, I notice how much these models apply to my own life. This degree has felt like a massive linear project. I set my "requirements" four years ago when I picked this major, and I’ve moved through the phases ever since. But humans aren’t static systems. My interests and understanding of the world have shifted, yet the academic life cycle rarely allows a pivot.</p>

    <p>I wonder if higher education could benefit from a more iterative approach. We are forced to commit before truly understanding the "system" we’re entering. By graduation, we often realize our initial career design was based on incomplete data. This creates stress. We are expected to deliver a perfect "system" of a career without prototyping or incremental testing. The same humans who struggle with rigid educational cycles will go on to manage engineering projects. If we can’t handle our own life cycles flexibly, we’ll struggle to do it for our teams.</p>

    <h2>The Deep Reality of Accountability Distribution</h2>

    <p>Life cycle models also shape accountability, both professional and emotional. Alami and Ernst (2024) distinguish institutionalized and grassroots accountability. Institutionalized accountability comes from formal processes like reviews and milestones, common in linear models. It relies on carrots and sticks and often fosters a culture of fear.</p>

    <p>Grassroots accountability arises organically, driven by peers and intrinsic commitment. Iterative and incremental models foster this, requiring constant collaboration. In short cycles, you cannot hide behind a document. You are accountable to the person next to you. Focus shifts from "who messed up" to "how do we fix this together." This shared responsibility is far healthier long-term than the looming single deadline of a waterfall model.</p>

    <h2>The Ethical Dimension: Responsibility and Fairness</h2>

    <p>Choosing a life cycle model has ethical implications. It can shield people from unrealistic expectations or set them up for impossible ones. It decides who bears the impact when circumstances change. Are risks shared fairly, or does one person absorb most consequences? Do people have agency, or are they blindsided by surprises?</p>

    <p>A responsible systems engineer asks not just if a model delivers the project, but if it treats people justly. A rigid linear approach in a high-uncertainty project is arguably unethical. It forces a team to follow a plan everyone knows is flawed. A good engineer balances stakeholder needs with the mental and emotional well-being of the team (Ramos, Ferreira, & Barceló, 2011).</p>

    <h2>The Broader Lesson for Future Engineers</h2>

    <p>No life cycle model is universally best. Each fits different human and project needs. The wisest engineers match their model to the realities and values of the people involved. Projects are as much about human hopes and fears as technical challenges. Success is not just about meeting requirements. It’s about whether the team survives and thrives.</p>

    <p>As I get ready to graduate, I take this lesson forward. The frameworks I choose will define the daily lives of my future colleagues. Life cycle models are more than textbook diagrams—they are promises about how we will treat each other when things go wrong. If we focus only on technical efficiency, we ignore the most important system component: the human being. Prioritizing learning, psychological safety, and shared accountability creates systems that are technically strong and humanly sustainable.</p>

    <footer class="article-footer">
        <h3>Sources</h3>
        <p>Holt, J., & Weilkiens,T. (2023). <em>Systems Engineering Demystified</em> (2nd ed.). Packt Publishing.</p>
        <p>Edmondson, A. (1999). Psychological safety and learning behavior in work teams. <em>Administrative Science Quarterly</em>.</p>
        <p>Alami, A., & Ernst, N. (2024). Understanding the building blocks of accountability in software engineering. <em>2024 IEEE/ACM 17th International Conference</em>.</p>
        <p>Ramos, A. L., Ferreira, J.V., & Barceló, J. (2011). <em>Model-based systems engineering tools</em>. SciTePress.</p>
    </footer>
</article>

</body>
         <!-- Module 2.1 Blog Post Starts here -->

         <!-- Module 2.2 Blog Post Starts here -->

         <!-- Module 2.3 Blog Post Starts here -->

         <!-- Module 3.1 Blog Post Starts here -->

         <!-- Module 3.2 Blog Post Starts here -->

      </main>
<!-- Page Footer  -->
      <footer class="page-footer">
       <p>Copyright &copy; 2025</p>
      </footer>
   </body>
</html>