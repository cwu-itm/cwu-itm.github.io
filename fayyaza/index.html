<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <link href="css/styles.css" rel="stylesheet">
      <title>Aroona Fayyaz | Portfolio</title>
   </head>
   <body>
<!-- Page Header  -->
      <header class="page-header">
         <h1>Portfolio</h1>
      </header>
<!-- Main Content Area  -->
      <main>
         <!-- Module 1.1 Blog Post Starts Here-->
         <article>
            <header class="article-header">
               <h2>Module 1.1 Blog Post</h2>
            </header>
            <h3>Systems Thinking: Understanding Complexity in Digital Environments</h3>
            <p>In our increasingly interconnected world, the challenges we face rarely exist in isolation. Whether designing a software application, managing a network infrastructure, or developing a complex digital system, the traditional approach of breaking problems into isolated pieces often falls short. This is where systems thinking becomes essential. Systems thinking represents a fundamental shift in how we perceive, analyze, and solve problems—particularly in the realm of digital systems where complexity, interdependencies, and emergent behaviors are the norm rather than the exception.</p>

            <p>Systems thinking is a comprehensive approach to understanding how different components within a system interact and influence each other to create behaviors and outcomes that cannot be understood by examining individual parts alone. Rather than viewing elements in isolation, systems thinking recognizes that the relationships between components are just as important—if not more so—than the components themselves. In my own words, systems thinking is the practice of stepping back to see the forest instead of just the trees, understanding that the forest has properties and behaviors that individual trees do not possess.</p>

            <p>Holt (2023) articulates this principle clearly in <em>Systems Engineering Demystified</em> when describing systems theory: “The main tenet of systems theory is that it is a conceptual framework based on the principle that the component parts of a system can best be understood in the context of the relationships with each other and with other systems, rather than in isolation” (p. 2). This foundational principle emphasizes that systems thinking is not merely about cataloging parts but about comprehending the dynamic interactions that create system-level behaviors.</p>

            <p>Arnold and Wade (2015) expand on this definition by identifying systems thinking as “a set of synergistic analytic skills used to improve the capability of identifying and understanding systems, predicting their behaviors, and devising modifications to them in order to produce desired effects” (p. 675). This definition highlights that systems thinking is both analytical and practical—it provides tools for understanding current system states while enabling us to envision and implement meaningful changes.</p>

            <p>What distinguishes systems thinking from other analytical approaches is its emphasis on feedback loops, emergent properties, and boundaries. Feedback loops represent how outputs of a system can circle back to influence inputs, creating either reinforcing or balancing cycles. Emergent properties are characteristics that arise from the interaction of system components but are not present in any individual component. Boundaries define what is included within the system and what exists in the external environment, though systems thinkers recognize these boundaries are often conceptual rather than physical.</p>

            <p>To deeply appreciate systems thinking, we must contrast it with the more traditional linear thinking that has dominated Western problem-solving approaches for centuries. Linear thinking, rooted in reductionist philosophy, assumes that complex problems can be solved by breaking them down into smaller, manageable pieces, solving each piece independently, and then reassembling the solutions. This approach follows a straight line from cause to effect.</p>

            <p>Linear thinking has served us well in many domains, particularly in mechanical systems and straightforward cause-and-effect relationships. However, it fundamentally fails when applied to complex systems where multiple variables interact simultaneously and outcomes depend on context and timing. Meadows (2008) notes that “we can’t impose our will on a system. We can listen to what the system tells us and discover how its properties and our values can work together to bring forth something much better than could ever be produced by our will alone” (p. 170).</p>

            <p>The differences between these two approaches manifest in several key ways. Linear thinking focuses on individual components, while systems thinking examines relationships and interactions. Linear thinking assumes stability and predictability, whereas systems thinking acknowledges dynamic behavior and emergence. Linear thinking seeks single causes and solutions, while systems thinking recognizes multiple causation and leverage points.</p>

            <p>Model-Based Systems Engineering (MBSE) provides a structured way to apply systems thinking to complex digital systems. Holt (2023) describes MBSE as “a multidisciplinary, common-sense approach that enables the realization of successful systems” (p. 14). Models simplify reality by capturing essential system properties while managing complexity through abstraction.</p>

            <p>As Holt (2023) explains, “the Model must be a simplification of the System and, therefore, there will be information associated with the System that is not contained in the Model” (p. 56). This selective abstraction allows stakeholders to understand system behavior without becoming overwhelmed by unnecessary detail.</p>

            <p>Models support visualization, stakeholder communication, consistency checking, impact analysis, and early issue detection. They provide a shared language across disciplines and allow engineers to simulate system behavior before implementation, reducing risk and cost.</p>

            <p>The shift from linear thinking to systems thinking, supported by model-based approaches, represents more than a technical methodology—it reflects a fundamental change in how we understand and interact with complexity. In digital environments where systems are interconnected and evolving, this shift is essential.</p>

            <p>Systems thinking recognizes that system behaviors emerge from interactions rather than isolated components. In complex digital systems, MBSE operationalizes this perspective by creating manageable abstractions that support communication, analysis, and implementation. As digital systems continue to grow in complexity and importance, systems thinking and model-based engineering become not just beneficial, but essential.</p>
            
            <footer class="article-footer">
                <h3>Sources:</h3>
                <ol>
                  <li>Arnold, R. D., &amp; Wade, J. P. (2015). A definition of systems thinking: A systems approach. <em>Procedia Computer Science, 44</em>, 669–678. https://doi.org/10.1016/j.procs.2015.03.050</li>
                  <li>Holt, J. (2023). <em>Systems engineering demystified</em> (2nd ed.). Packt Publishing.</li>
                  <li>Meadows, D. H. (2008). <em>Thinking in systems: A primer</em>. Chelsea Green Publishing.</li>
                </ol>
            </footer>
         </article>

         <!-- Module 1.2 Blog Post Starts here -->

         <article>
            <header>
               <h1>Thinking in Life Cycles: The Hidden Architecture of Digital Evolution</h1>
            </header>
            <p>
            When we interact with our smartphones, stream content, or rely on cloud infrastructure, we rarely consider the intricate journey these systems undertake from conception to retirement. Yet understanding this journey—the systems development life cycle—is fundamental to creating sustainable, successful digital systems. As someone navigating the intersection of technology and design, I have come to realize that thinking in life cycles is not just a project management framework; it represents a fundamental shift in how we approach the creation and evolution of complex systems.
            </p>
            <h2>Understanding the Systems Development Life Cycle</h2>
            <p>
            The systems development life cycle (SDLC) represents a structured approach to understanding how systems evolve over time. According to Holt (2024), a system life cycle “describes the evolution of a system” through distinct stages that guide development from initial need identification through final retirement. Rather than viewing systems as static products, the life cycle perspective recognizes them as living entities that grow, adapt, and eventually complete their usefulness.
            </p>
            <p>At its core, the SDLC encompasses six fundamental stages: Conception, Development, Production, Utilization, Support, and Retirement (Holt, 2024). The Conception stage focuses on identifying and defining system needs through stakeholder analysis. Development explores potential solutions and prototypes. Production creates the actual system and conducts verification and validation testing. Utilization describes active use by end users, while Support provides maintenance and error-reporting services that run parallel to utilization. Finally, Retirement ensures safe and secure decommissioning.
            </p>
            <p>This framework is powerful because it recognizes that different aspects of a system’s existence require different considerations. As Blanchard and Fabrycky (2014) explain, the life-cycle approach provides a necessary framework for addressing all aspects of a system from “cradle to grave.” This holistic view prevents the common pitfall of optimizing one stage—often production—while creating downstream problems in support or retirement.
            </p>
            <h2>The Linear Life Cycle Model: Clarity Through Sequential Progression</h2>
            <p>The linear life cycle model, commonly illustrated by Royce’s Waterfall Model, executes stages in a strict sequential order where each stage must be completed before the next begins. Holt (2024) explains that in this model, each stage is executed in a specific order with little to no opportunity to return to a previous stage.
            </p>
            <p>The primary advantage of the linear approach lies in its simplicity and predictability. When requirements are well understood and unlikely to change, this model offers clear milestones and straightforward resource management. Sommerville (2016) notes that plan-driven processes are most effective when requirements are stable and technologies are well understood. For smaller, clearly scoped digital projects, the linear model provides an intuitive and manageable path forward.
            </p>
            <p>However, the rigidity of the linear model becomes a serious limitation in complex and uncertain environments. Modern digital systems rarely have fully defined requirements at the outset. User expectations evolve, technologies shift, and markets change. Because earlier stages cannot easily be revisited, discovering major issues late in development can be costly and disruptive. As Holt (2024) observes, the linear life cycle model is not well suited for complex systems where needs are likely to change.
            </p>
            <h2>The Iterative Life Cycle Model: Learning Through Repetition</h2>
            <p>The iterative life cycle model addresses uncertainty by allowing multiple passes through the life cycle stages. Instead of a single progression from conception to production, development occurs through repeated iterations, each producing a functional version of the system (Holt, 2024).
            </p>
            <p>This approach mirrors how learning naturally occurs. Each iteration provides an opportunity to test assumptions, gather feedback, and refine solutions. Agile development practices exemplify this model, enabling organizations to respond quickly to change. Companies such as Spotify and Netflix rely on frequent iterations to continuously improve their platforms and user experiences.
            </p>
            <p>Iterative models are particularly effective for exploratory digital projects where solutions are not immediately clear. However, rapid iteration can introduce challenges, including fragmented architecture and technical debt. Holt (2024) emphasizes that while iterative approaches are compatible with model-based systems engineering, maintaining architectural discipline is essential to ensure long-term system integrity.
            </p>
            <h2>The Incremental Life Cycle Model: Building in Stages</h2>
            <p>The incremental life cycle model offers a balance between linear and iterative approaches. After defining overall requirements during conception, the system is delivered in functional subsets or increments (Holt, 2024). Each increment adds value while the system remains operational.
            </p>
            <p>This approach enables earlier delivery of core functionality and provides stakeholders with visible progress. It also allows organizations to distribute costs over time and adapt future increments based on feedback from earlier releases. Many large-scale digital platforms follow incremental strategies, gradually expanding features while maintaining service continuity.
            </p>
            <p>Despite its benefits, incremental development requires careful architectural planning. Not all systems can be easily divided into independent components, and poorly planned increments may require costly redesigns later. Successful incremental models depend on a strong foundational architecture capable of supporting future growth.
            </p>
            <h2>Why Life Cycle Thinking Matters for Digital Systems</h2>
            <p>Thinking in life cycles fundamentally reshapes how digital systems are designed and managed. Holt (2024) identifies complexity—arising from interactions between system elements—as a core challenge in systems engineering. Life cycle thinking helps manage this complexity by providing context for when and how interactions occur.
            </p>
            <p>One key benefit is sustainability. Designing systems with their entire life span in mind encourages maintainability, adaptability, and responsible retirement planning. This long-term perspective helps reduce technical debt and supports both economic and technical sustainability. The rise of DevOps practices reflects this mindset by recognizing that development, deployment, and operations are interconnected stages rather than isolated phases.
            </p>
            <p>Life cycle thinking also improves stakeholder communication. Different stakeholders engage with systems at different stages, from executives during conception to support teams during long-term utilization. Explicitly defining life cycle stages creates a shared language that helps align expectations and decision-making (Holt, 2024).
            </p>
            <p>Additionally, a life cycle perspective reveals hidden costs and opportunities. Initial development costs represent only a fraction of total ownership expenses. Training, maintenance, integration, and eventual migration all contribute significantly. Understanding these factors enables more informed technology investment decisions and highlights opportunities for innovation during later stages such as utilization and support.
            </p>
            <h2>The Meta-Pattern: Interacting Life Cycles</h2>
            <p>Holt (2024) further emphasizes that multiple life cycles often operate simultaneously and interact with one another. A system life cycle may intersect with procurement, technology, asset, and regulatory life cycles. These interactions create critical decision points, especially when one component reaches decline while others remain active.
            </p>
            <p>In digital ecosystems, these interactions are particularly complex. Applications depend on platforms, cloud infrastructure, and third-party libraries that evolve independently. Security vulnerabilities, platform deprecations, or regulatory changes can abruptly alter a system’s intended life span. Recognizing and planning for these interactions is essential for resilient system design.
            </p>
            <h2>Conclusion: Life Cycles as Temporal Architecture</h2>
            <p>Thinking in life cycles represents a form of temporal architecture—designing systems not only for their current state but for their future evolution and eventual retirement. This perspective complements traditional structural architecture by accounting for time as a critical design dimension.
            </p>
            <p>No single life cycle model is universally superior. Linear models suit stable, well-defined projects, iterative models support exploration and learning, and incremental models enable staged value delivery. Most complex digital systems benefit from a hybrid approach that adapts life cycle execution to context.
            </p>
            <p>What remains constant is the recognition that systems live, evolve, and eventually fulfill their purpose. By embracing life cycle thinking from the outset, we can design digital systems that are sustainable, adaptable, and responsible throughout their entire existence. In an industry often driven by short-term priorities, this long-term perspective is both a best practice and an ethical responsibility.
            </p>

            <footer class="article-footer">
                <h3>References</h3>
                <p>Blanchard, B. S., &amp; Fabrycky, W. J. (2014). <em>Systems engineering and analysis</em> (5th ed.). Pearson.
                </p>
                <p>Holt, J. (2024). <em>Systems engineering demystified: Apply modern, model-based systems engineering techniques to build complex systems</em> (2nd ed.). Packt Publishing.
                </p>
                <p>Sommerville, I. (2016). <em>Software engineering</em> (10th ed.). Pearson.
                </p>
            </footer>
        </article>

<!-- Module 1.2 Blog Ends Here -->


         <!-- Module 2.1 Blog Post Starts here -->
         <article>
            <header>
               <h1>First: Designing with People at the Center</h1> 
            </header>
            <h2>Introduction: A Simple Idea with Deep Roots</h2>
            <p>
            There is a deceptively simple idea at the heart of modern design: before you build anything,
            you should understand the people who will use it. This sounds obvious—too obvious to mention.
            But the history of technology is littered with products that were engineered brilliantly and
            yet failed spectacularly because nobody stopped to ask whether a real human being could actually
            use them comfortably, intuitively, or even at all. Human-centered design (HCD) is the discipline
            that exists to fix that problem. It is not a single method or a checklist you run through at the
            end of a project. It is a way of thinking—a commitment to keeping people at the center of every
            decision you make, from the very first spark of an idea to the final product sitting in someone's hands.
            </p>

            <p>
            This post explores what that commitment looks like in practice. We will walk through two of the
            most influential frameworks for human-centered design—the ISO 9241-210 standard and IDEO's Field
            Guide to Human-Centered Design—and examine how they each approach the challenge of designing for people.
            Along the way, we will pull in some of the foundational ideas from UX research: how human perception
            and visual hierarchy shape the way we interact with interfaces, and how well-established design laws
            like Jakob's Law, Fitts's Law, Hick's Law, and Miller's Law quietly govern so much of what makes a design
            feel effortless or frustrating. The goal is not to memorize a framework. The goal is to understand why
            designing for people is harder—and more important—than it might first appear.
            </p>

            <h2>What Is Human-Centered Design, Really?</h2>

            <p>
            Human-centered design is, at its core, a process built around a single conviction: the people who
            will use your product are your most valuable source of information. Not your engineering team.
            Not your marketing department. Not even your own intuition as a designer. The people. Their needs,
            their contexts, their struggles, and their expectations—these are the raw material from which good
            design is made.
            </p>

            <p>
            The ISO 9241-210 standard, first published in 1999, is one of the earliest and most enduring attempts
            to formalize this idea into a repeatable process. It defines human-centered design as an approach that
            ensures "suitable designs can be created" by drawing on knowledge from ergonomics, human factors, and
            usability, all while keeping the user at the center (Wickens et al., 2004). The standard lays out a
            six-step cyclical process: plan the HCD process, understand and specify the context of use, specify
            user requirements, produce design solutions, evaluate the design against those requirements, and
            then iterate. That last word—iterate—is doing enormous work in that sentence. The whole point is that
            you do not get it right the first time. You learn, you adjust, and you go around again.
            </p>

            <p>
            IDEO's Field Guide to Human-Centered Design takes a different tone but arrives at a remarkably similar
            place. Where ISO 9241-210 is methodical and standards-driven, IDEO's approach is more explicitly
            creative and emotionally grounded. The Field Guide organizes the design process into three broad
            phases—Inspiration, Ideation, and Implementation—and wraps the whole thing in a set of mindsets that
            designers are encouraged to cultivate: empathy, optimism, iteration, creative confidence, a bias
            toward making, an ability to embrace ambiguity, and a willingness to learn from failure (IDEO, 2015).
            These are not just nice-sounding principles. They are, according to IDEO, the actual cognitive and
            emotional habits that separate designers who innovate from those who simply produce.
            </p>

            <p>
            What is striking is how much these two frameworks agree on, despite their very different styles.
            Both insist that understanding users is not optional—it is the foundation. Both emphasize iteration
            as essential rather than optional. And both acknowledge that designing for people is genuinely
            difficult, not because the methods are complicated, but because truly listening to and learning
            from other human beings requires patience, humility, and a willingness to be surprised by what you find.
            </p>

            <h2>Comparing the Two Frameworks: Process vs. Mindset</h2>

            <p>
            If you placed ISO 9241-210 and the IDEO Field Guide side by side, the most obvious difference is
            structural. The ISO standard gives you a clear, step-by-step loop. It tells you to plan, research,
            define requirements, design, evaluate, and repeat. It is engineered for organizations that need
            a repeatable, documentable process—something you can integrate into a waterfall or agile development
            cycle and point to as evidence that you took usability seriously (Wickens et al., 2004). There is
            real value in that kind of rigor. It keeps teams aligned. It creates checkpoints. It makes it harder
            to skip the parts of the process that feel slow or uncomfortable.
            </p>

            <p>
            IDEO, on the other hand, does not give you a rigid loop. It gives you a philosophy. The three phases—
            Inspiration, Ideation, Implementation—are deliberately broad, and the Field Guide is upfront about
            the fact that "human-centered design isn't a perfectly linear process, and each project invariably
            has its own contours and character" (IDEO, 2015). The emphasis falls less on following steps and more
            on cultivating a certain way of being in the design process. You should be curious. You should be
            comfortable not knowing the answer. You should prototype early and often, not because you expect your
            first attempt to work, but because the act of making something tangible is itself a form of learning.
            </p>

            <p>
            This difference matters because it reflects two different assumptions about where good design comes
            from. The ISO framework assumes that if you follow the right process with sufficient discipline, you
            will arrive at a usable product. IDEO's framework assumes that good design also requires a certain
            kind of creative courage—the willingness to sit with uncertainty, generate ideas that might fail,
            and treat failure not as a setback but as information. In practice, the strongest design teams draw
            on both. They use structured processes to stay organized and accountable, and they cultivate open,
            experimental mindsets to keep their work genuinely innovative.
            </p>

            <p>
            One area where the two frameworks converge with particular force is on the question of empathy.
            ISO 9241-210 requires that design be based on "a comprehensive understanding of the users, tasks,
            and working environment" and that users be directly involved in the design and development process
            (Wickens et al., 2004). IDEO goes further, framing empathy as one of the seven core mindsets of a
            human-centered designer. "Empathy is the capacity to step into other people's shoes, to understand
            their lives, and start to solve problems from their perspectives," the Field Guide states (IDEO, 2015).
            Empathy, in this view, is not just a research technique. It is a disposition—a way of approaching the
            world that makes better design possible.
            </p>

            <h2>Perception, Visual Hierarchy, and Why They Matter</h2>

            <p>
            Understanding how people think and perceive the world is not an abstract academic exercise. It is
            directly, practically relevant to every design decision you make. Human beings do not experience
            interfaces the way a computer does—pixel by pixel, element by element, in a neutral and objective
            way. We scan. We prioritize. We make rapid judgments based on visual cues, and we fill in gaps with
            assumptions drawn from past experience. If a design does not account for these tendencies, it will
            feel confusing, overwhelming, or simply wrong—even if every individual element is technically
            well-designed.
            </p>

            <p>
            Visual hierarchy is one of the most fundamental tools designers use to work with human perception
            rather than against it. It is the practice of organizing elements on a page or screen so that the
            most important information stands out and the least important information recedes. This is achieved
            through size, color, contrast, spacing, and position. A large, bold headline draws the eye before a
            smaller block of body text. A brightly colored button commands attention before a muted link. These
            are not arbitrary aesthetic choices. They are decisions rooted in how human visual processing
            actually works—how we distinguish figure from ground, how we group related elements, and how we
            direct our attention across a complex visual field (Wickens et al., 2004).
            </p>

            <p>
            When visual hierarchy is done well, the user barely notices it. The interface just feels intuitive.
            The right information is where they expect it to be. The next action they need to take is obvious.
            When it is done poorly, the opposite happens: the user has to work to figure out what is important,
            where to look, and what to do next. This friction—small as it might seem in any single moment—
            accumulates over time and degrades the entire experience.
            </p>

            <h2>The Four UX Laws and What They Teach Us</h2>

            <p>
            Several well-known principles from UX research help explain why certain design patterns feel so
            natural and others feel so clunky. Four of them are particularly relevant to the ideas we have
            been discussing.
            </p>

            <p>
            Jakob's Law is perhaps the most widely cited. It states that users prefer interfaces that work the
            way they already expect them to work, based on their experience with other products. This is not
            a suggestion—it is a description of how human cognition works. We build mental models of how things
            should behave, and when an interface violates those models, we experience friction and confusion.
            Jakob Nielsen, the usability researcher who popularized this principle, argues that designers should
            leverage existing conventions rather than constantly reinventing the wheel (Nielsen, 2000). This
            does not mean design should be boring or derivative. It means that novelty should be introduced
            thoughtfully, in ways that do not force users to abandon everything they already know.
            </p>

            <p>
            Fitts's Law comes from a different tradition—motor psychology rather than cognitive psychology—but
            its implications for design are just as significant. It states that the time it takes to move to a
            target is a function of the distance to that target and the size of the target itself. In practical
            terms, this means that important buttons and interactive elements should be large and easy to reach.
            A tiny, hard-to-click link buried in a corner of the screen is not just an aesthetic failure. It is
            a violation of a fundamental principle of human movement and interaction (Wickens et al., 2004).
            Good designers internalize Fitts's Law almost instinctively—they make the things users need to click
            bigger, closer, and more prominent.
            </p>

            <p>
            Hick's Law tells us that the time it takes a person to make a decision increases with the number of
            choices available to them. This is why overwhelming a user with too many options at once is almost
            always a mistake. A navigation menu with thirty items is harder to use than one with five, not
            because the user cannot read all thirty, but because the cognitive load of evaluating thirty options
            slows them down and increases the chance of error. Designers use Hick's Law to justify simplification—
            reducing menus, breaking complex tasks into smaller steps, and presenting information in manageable
            chunks rather than all at once (Wickens et al., 2004).
            </p>

            <p>
            Miller's Law rounds out the picture by reminding us that human working memory is limited. The classic
            formulation is that people can hold roughly seven items—plus or minus two—in working memory at any
            given time. This is why long lists, dense forms, and information-heavy screens so often feel
            overwhelming. It is not that users are not paying attention. It is that their cognitive capacity
            has a hard ceiling, and designs that exceed it will inevitably feel stressful and error-prone.
            Chunking information—grouping related items together so they can be processed as a single unit—is
            one of the most effective ways to work within this constraint (Wickens et al., 2004).
            </p>

            <p>
            Taken together, these four laws paint a clear picture: human beings are not infinitely flexible
            processors. They have habits, expectations, physical limitations, and cognitive constraints.
            Design that respects these realities will feel effortless. Design that ignores them will feel
            broken, no matter how visually polished it might be.
            </p>

            <h2>Bringing It All Together: Why "Human First" Is Not Just a Slogan</h2>

            <p>
            The phrase "human first" gets used a lot in design circles, and like any phrase that gets used a
            lot, it can start to feel hollow. But when you actually trace what it means—when you look at the
            ISO 9241-210 standard, when you read IDEO's Field Guide, when you study how perception and cognition
            shape every interaction a person has with a screen—it turns out to be one of the most consequential
            commitments a design team can make.
            </p>

            <p>
            Designing for people means accepting that your users are not a blank slate. They come with mental
            models built from years of experience. They have limited attention, limited working memory, and
            limited patience. They scan before they read. They make judgments in fractions of a second based on
            visual cues. They expect things to work the way other things they have used before have worked. And
            they will tell you, if you bother to ask, exactly what is confusing, frustrating, or broken about
            your product—but only if you create the conditions for them to do so.
            </p>

            <p>
            Both the ISO standard and IDEO's Field Guide are, at their heart, frameworks for creating those
            conditions. They are different in tone, structure, and emphasis, but they share a common conviction:
            that the best designs are not the ones that are most technically impressive or visually stunning.
            They are the ones that make the most sense to the people who use them. The ones that feel obvious.
            The ones that get out of the way and let people do what they came to do.
            </p>

            <p>
            That is what "human first" actually means. Not a buzzword. Not a marketing tagline. A design
            philosophy that, when practiced with discipline and genuine curiosity, produces work that lasts.
            </p>
            
            <footer class="article-footer">
                <h3>References</h3>
                <p>
            IDEO. (2015). <em>The field guide to human-centered design</em> (1st ed.). IDEO.org.
            </p>

            <p>
            Nielsen, J. (2000). Jakob's law. Nielsen Norman Group.
            https://www.nngroup.com/articles/jakobs-law-compat/
            </p>

            <p>
            Wickens, C. D., Hollands, J. G., Banbury, S., & Parasuraman, R. (2004).
            <em>Attentional resources</em> (2nd ed.). Routledge.
            </p>

            <p>
            Wickens, C. D., Hollands, J. G., Banbury, S., & Parasuraman, R. (2004).
            <em>Engineering psychology and human performance</em> (5th ed.). Routledge.
            </p>
            </footer>
        </article>
         


         <!-- Module 2.2 Blog Post Starts here -->

         <!-- Module 2.3 Blog Post Starts here -->

         <!-- Module 3.1 Blog Post Starts here -->

         <!-- Module 3.2 Blog Post Starts here -->

      </main>
<!-- Page Footer  -->
      <footer class="page-footer">
       <p>Copyright &copy; 2025</p>
      </footer>
   </body>
</html>