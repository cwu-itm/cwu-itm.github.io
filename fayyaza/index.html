<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <link href="css/styles.css" rel="stylesheet">
      <title>Aroona Fayyaz | Portfolio</title>
   </head>
   <body>
<!-- Page Header  -->
      <header class="page-header">
         <h1>Portfolio</h1>
      </header>
<!-- Main Content Area  -->
      <main>
         <!-- Module 1.1 Blog Post Starts Here-->
         <article>
            <header class="article-header">
               <h2>Module 1.1 Blog Post</h2>
            </header>
            <h3>Systems Thinking: Understanding Complexity in Digital Environments</h3>
            <p>In our increasingly interconnected world, the challenges we face rarely exist in isolation. Whether designing a software application, managing a network infrastructure, or developing a complex digital system, the traditional approach of breaking problems into isolated pieces often falls short. This is where systems thinking becomes essential. Systems thinking represents a fundamental shift in how we perceive, analyze, and solve problems—particularly in the realm of digital systems where complexity, interdependencies, and emergent behaviors are the norm rather than the exception.</p>

            <p>Systems thinking is a comprehensive approach to understanding how different components within a system interact and influence each other to create behaviors and outcomes that cannot be understood by examining individual parts alone. Rather than viewing elements in isolation, systems thinking recognizes that the relationships between components are just as important—if not more so—than the components themselves. In my own words, systems thinking is the practice of stepping back to see the forest instead of just the trees, understanding that the forest has properties and behaviors that individual trees do not possess.</p>

            <p>Holt (2023) articulates this principle clearly in <em>Systems Engineering Demystified</em> when describing systems theory: “The main tenet of systems theory is that it is a conceptual framework based on the principle that the component parts of a system can best be understood in the context of the relationships with each other and with other systems, rather than in isolation” (p. 2). This foundational principle emphasizes that systems thinking is not merely about cataloging parts but about comprehending the dynamic interactions that create system-level behaviors.</p>

            <p>Arnold and Wade (2015) expand on this definition by identifying systems thinking as “a set of synergistic analytic skills used to improve the capability of identifying and understanding systems, predicting their behaviors, and devising modifications to them in order to produce desired effects” (p. 675). This definition highlights that systems thinking is both analytical and practical—it provides tools for understanding current system states while enabling us to envision and implement meaningful changes.</p>

            <p>What distinguishes systems thinking from other analytical approaches is its emphasis on feedback loops, emergent properties, and boundaries. Feedback loops represent how outputs of a system can circle back to influence inputs, creating either reinforcing or balancing cycles. Emergent properties are characteristics that arise from the interaction of system components but are not present in any individual component. Boundaries define what is included within the system and what exists in the external environment, though systems thinkers recognize these boundaries are often conceptual rather than physical.</p>

            <p>To deeply appreciate systems thinking, we must contrast it with the more traditional linear thinking that has dominated Western problem-solving approaches for centuries. Linear thinking, rooted in reductionist philosophy, assumes that complex problems can be solved by breaking them down into smaller, manageable pieces, solving each piece independently, and then reassembling the solutions. This approach follows a straight line from cause to effect.</p>

            <p>Linear thinking has served us well in many domains, particularly in mechanical systems and straightforward cause-and-effect relationships. However, it fundamentally fails when applied to complex systems where multiple variables interact simultaneously and outcomes depend on context and timing. Meadows (2008) notes that “we can’t impose our will on a system. We can listen to what the system tells us and discover how its properties and our values can work together to bring forth something much better than could ever be produced by our will alone” (p. 170).</p>

            <p>The differences between these two approaches manifest in several key ways. Linear thinking focuses on individual components, while systems thinking examines relationships and interactions. Linear thinking assumes stability and predictability, whereas systems thinking acknowledges dynamic behavior and emergence. Linear thinking seeks single causes and solutions, while systems thinking recognizes multiple causation and leverage points.</p>

            <p>Model-Based Systems Engineering (MBSE) provides a structured way to apply systems thinking to complex digital systems. Holt (2023) describes MBSE as “a multidisciplinary, common-sense approach that enables the realization of successful systems” (p. 14). Models simplify reality by capturing essential system properties while managing complexity through abstraction.</p>

            <p>As Holt (2023) explains, “the Model must be a simplification of the System and, therefore, there will be information associated with the System that is not contained in the Model” (p. 56). This selective abstraction allows stakeholders to understand system behavior without becoming overwhelmed by unnecessary detail.</p>

            <p>Models support visualization, stakeholder communication, consistency checking, impact analysis, and early issue detection. They provide a shared language across disciplines and allow engineers to simulate system behavior before implementation, reducing risk and cost.</p>

            <p>The shift from linear thinking to systems thinking, supported by model-based approaches, represents more than a technical methodology—it reflects a fundamental change in how we understand and interact with complexity. In digital environments where systems are interconnected and evolving, this shift is essential.</p>

            <p>Systems thinking recognizes that system behaviors emerge from interactions rather than isolated components. In complex digital systems, MBSE operationalizes this perspective by creating manageable abstractions that support communication, analysis, and implementation. As digital systems continue to grow in complexity and importance, systems thinking and model-based engineering become not just beneficial, but essential.</p>
            
            <footer class="article-footer">
                <h3>Sources:</h3>
                <ol>
                  <li>Arnold, R. D., &amp; Wade, J. P. (2015). A definition of systems thinking: A systems approach. <em>Procedia Computer Science, 44</em>, 669–678. https://doi.org/10.1016/j.procs.2015.03.050</li>
                  <li>Holt, J. (2023). <em>Systems engineering demystified</em> (2nd ed.). Packt Publishing.</li>
                  <li>Meadows, D. H. (2008). <em>Thinking in systems: A primer</em>. Chelsea Green Publishing.</li>
                </ol>
            </footer>
         </article>

         <!-- Module 1.2 Blog Post Starts here -->

         <article>
            <header>
               <h1>Thinking in Life Cycles: The Hidden Architecture of Digital Evolution</h1>
            </header>
            <p>
            When we interact with our smartphones, stream content, or rely on cloud infrastructure, we rarely consider the intricate journey these systems undertake from conception to retirement. Yet understanding this journey—the systems development life cycle—is fundamental to creating sustainable, successful digital systems. As someone navigating the intersection of technology and design, I have come to realize that thinking in life cycles is not just a project management framework; it represents a fundamental shift in how we approach the creation and evolution of complex systems.
            </p>
            <h2>Understanding the Systems Development Life Cycle</h2>
            <p>
            The systems development life cycle (SDLC) represents a structured approach to understanding how systems evolve over time. According to Holt (2024), a system life cycle “describes the evolution of a system” through distinct stages that guide development from initial need identification through final retirement. Rather than viewing systems as static products, the life cycle perspective recognizes them as living entities that grow, adapt, and eventually complete their usefulness.
            </p>
            <p>At its core, the SDLC encompasses six fundamental stages: Conception, Development, Production, Utilization, Support, and Retirement (Holt, 2024). The Conception stage focuses on identifying and defining system needs through stakeholder analysis. Development explores potential solutions and prototypes. Production creates the actual system and conducts verification and validation testing. Utilization describes active use by end users, while Support provides maintenance and error-reporting services that run parallel to utilization. Finally, Retirement ensures safe and secure decommissioning.
            </p>
            <p>This framework is powerful because it recognizes that different aspects of a system’s existence require different considerations. As Blanchard and Fabrycky (2014) explain, the life-cycle approach provides a necessary framework for addressing all aspects of a system from “cradle to grave.” This holistic view prevents the common pitfall of optimizing one stage—often production—while creating downstream problems in support or retirement.
            </p>
            <h2>The Linear Life Cycle Model: Clarity Through Sequential Progression</h2>
            <p>The linear life cycle model, commonly illustrated by Royce’s Waterfall Model, executes stages in a strict sequential order where each stage must be completed before the next begins. Holt (2024) explains that in this model, each stage is executed in a specific order with little to no opportunity to return to a previous stage.
            </p>
            <p>The primary advantage of the linear approach lies in its simplicity and predictability. When requirements are well understood and unlikely to change, this model offers clear milestones and straightforward resource management. Sommerville (2016) notes that plan-driven processes are most effective when requirements are stable and technologies are well understood. For smaller, clearly scoped digital projects, the linear model provides an intuitive and manageable path forward.
            </p>
            <p>However, the rigidity of the linear model becomes a serious limitation in complex and uncertain environments. Modern digital systems rarely have fully defined requirements at the outset. User expectations evolve, technologies shift, and markets change. Because earlier stages cannot easily be revisited, discovering major issues late in development can be costly and disruptive. As Holt (2024) observes, the linear life cycle model is not well suited for complex systems where needs are likely to change.
            </p>
            <h2>The Iterative Life Cycle Model: Learning Through Repetition</h2>
            <p>The iterative life cycle model addresses uncertainty by allowing multiple passes through the life cycle stages. Instead of a single progression from conception to production, development occurs through repeated iterations, each producing a functional version of the system (Holt, 2024).
            </p>
            <p>This approach mirrors how learning naturally occurs. Each iteration provides an opportunity to test assumptions, gather feedback, and refine solutions. Agile development practices exemplify this model, enabling organizations to respond quickly to change. Companies such as Spotify and Netflix rely on frequent iterations to continuously improve their platforms and user experiences.
            </p>
            <p>Iterative models are particularly effective for exploratory digital projects where solutions are not immediately clear. However, rapid iteration can introduce challenges, including fragmented architecture and technical debt. Holt (2024) emphasizes that while iterative approaches are compatible with model-based systems engineering, maintaining architectural discipline is essential to ensure long-term system integrity.
            </p>
            <h2>The Incremental Life Cycle Model: Building in Stages</h2>
            <p>The incremental life cycle model offers a balance between linear and iterative approaches. After defining overall requirements during conception, the system is delivered in functional subsets or increments (Holt, 2024). Each increment adds value while the system remains operational.
            </p>
            <p>This approach enables earlier delivery of core functionality and provides stakeholders with visible progress. It also allows organizations to distribute costs over time and adapt future increments based on feedback from earlier releases. Many large-scale digital platforms follow incremental strategies, gradually expanding features while maintaining service continuity.
            </p>
            <p>Despite its benefits, incremental development requires careful architectural planning. Not all systems can be easily divided into independent components, and poorly planned increments may require costly redesigns later. Successful incremental models depend on a strong foundational architecture capable of supporting future growth.
            </p>
            <h2>Why Life Cycle Thinking Matters for Digital Systems</h2>
            <p>Thinking in life cycles fundamentally reshapes how digital systems are designed and managed. Holt (2024) identifies complexity—arising from interactions between system elements—as a core challenge in systems engineering. Life cycle thinking helps manage this complexity by providing context for when and how interactions occur.
            </p>
            <p>One key benefit is sustainability. Designing systems with their entire life span in mind encourages maintainability, adaptability, and responsible retirement planning. This long-term perspective helps reduce technical debt and supports both economic and technical sustainability. The rise of DevOps practices reflects this mindset by recognizing that development, deployment, and operations are interconnected stages rather than isolated phases.
            </p>
            <p>Life cycle thinking also improves stakeholder communication. Different stakeholders engage with systems at different stages, from executives during conception to support teams during long-term utilization. Explicitly defining life cycle stages creates a shared language that helps align expectations and decision-making (Holt, 2024).
            </p>
            <p>Additionally, a life cycle perspective reveals hidden costs and opportunities. Initial development costs represent only a fraction of total ownership expenses. Training, maintenance, integration, and eventual migration all contribute significantly. Understanding these factors enables more informed technology investment decisions and highlights opportunities for innovation during later stages such as utilization and support.
            </p>
            <h2>The Meta-Pattern: Interacting Life Cycles</h2>
            <p>Holt (2024) further emphasizes that multiple life cycles often operate simultaneously and interact with one another. A system life cycle may intersect with procurement, technology, asset, and regulatory life cycles. These interactions create critical decision points, especially when one component reaches decline while others remain active.
            </p>
            <p>In digital ecosystems, these interactions are particularly complex. Applications depend on platforms, cloud infrastructure, and third-party libraries that evolve independently. Security vulnerabilities, platform deprecations, or regulatory changes can abruptly alter a system’s intended life span. Recognizing and planning for these interactions is essential for resilient system design.
            </p>
            <h2>Conclusion: Life Cycles as Temporal Architecture</h2>
            <p>Thinking in life cycles represents a form of temporal architecture—designing systems not only for their current state but for their future evolution and eventual retirement. This perspective complements traditional structural architecture by accounting for time as a critical design dimension.
            </p>
            <p>No single life cycle model is universally superior. Linear models suit stable, well-defined projects, iterative models support exploration and learning, and incremental models enable staged value delivery. Most complex digital systems benefit from a hybrid approach that adapts life cycle execution to context.
            </p>
            <p>What remains constant is the recognition that systems live, evolve, and eventually fulfill their purpose. By embracing life cycle thinking from the outset, we can design digital systems that are sustainable, adaptable, and responsible throughout their entire existence. In an industry often driven by short-term priorities, this long-term perspective is both a best practice and an ethical responsibility.
            </p>

            <footer class="article-footer">
                <h3>References</h3>
                <p>Blanchard, B. S., &amp; Fabrycky, W. J. (2014). <em>Systems engineering and analysis</em> (5th ed.). Pearson.
                </p>
                <p>Holt, J. (2024). <em>Systems engineering demystified: Apply modern, model-based systems engineering techniques to build complex systems</em> (2nd ed.). Packt Publishing.
                </p>
                <p>Sommerville, I. (2016). <em>Software engineering</em> (10th ed.). Pearson.
                </p>
            </footer>
        </article>

<!-- Module 1.2 Blog Ends Here -->


   


         <!-- Module 2.1 Blog Post Starts here -->
        <article>
          <!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Human First: Designing with People at the Center</title>
</head>
<body>

    <h1>Human First: Designing with People at the Center</h1>

    <h2>Introduction: A Simple Idea with Deep Roots</h2>

    <p>
        There is a deceptively simple idea at the heart of modern design: before you build anything,
        you should understand the people who will use it. This sounds obvious—too obvious to mention.
        But the history of technology is littered with products that were engineered brilliantly and
        yet failed spectacularly because nobody stopped to ask whether a real human being could actually
        use them comfortably, intuitively, or even at all. Human-centered design (HCD) is the discipline
        that exists to fix that problem. It is not a single method or a checklist you run through at the
        end of a project. It is a way of thinking—a commitment to keeping people at the center of every
        decision you make, from the very first spark of an idea to the final product sitting in someone's hands.
    </p>

    <p>
        This post explores what that commitment looks like in practice. We will walk through two of the
        most influential frameworks for human-centered design—the ISO 9241-210 standard and IDEO's Field
        Guide to Human-Centered Design—and examine how they each approach the challenge of designing for people.
        Along the way, we will pull in some of the foundational ideas from UX research: how human perception
        and visual hierarchy shape the way we interact with interfaces, and how well-established design laws
        like Jakob's Law, Fitts's Law, Hick's Law, and Miller's Law quietly govern so much of what makes a design
        feel effortless or frustrating. The goal is not to memorize a framework. The goal is to understand why
        designing for people is harder—and more important—than it might first appear.
    </p>

    <h2>What Is Human-Centered Design, Really?</h2>

    <p>
        Human-centered design is, at its core, a process built around a single conviction: the people who
        will use your product are your most valuable source of information. Not your engineering team.
        Not your marketing department. Not even your own intuition as a designer. The people. Their needs,
        their contexts, their struggles, and their expectations—these are the raw material from which good
        design is made.
    </p>

    <p>
        The ISO 9241-210 standard, first published in 1999, is one of the earliest and most enduring attempts
        to formalize this idea into a repeatable process. It defines human-centered design as an approach that
        ensures "suitable designs can be created" by drawing on knowledge from ergonomics, human factors, and
        usability, all while keeping the user at the center (Wickens et al., 2004). The standard lays out a
        six-step cyclical process: plan the HCD process, understand and specify the context of use, specify
        user requirements, produce design solutions, evaluate the design against those requirements, and
        then iterate. That last word—iterate—is doing enormous work in that sentence. The whole point is that
        you do not get it right the first time. You learn, you adjust, and you go around again.
    </p>

    <p>
        IDEO's Field Guide to Human-Centered Design takes a different tone but arrives at a remarkably similar
        place. Where ISO 9241-210 is methodical and standards-driven, IDEO's approach is more explicitly
        creative and emotionally grounded. The Field Guide organizes the design process into three broad
        phases—Inspiration, Ideation, and Implementation—and wraps the whole thing in a set of mindsets that
        designers are encouraged to cultivate: empathy, optimism, iteration, creative confidence, a bias
        toward making, an ability to embrace ambiguity, and a willingness to learn from failure (IDEO, 2015).
        These are not just nice-sounding principles. They are, according to IDEO, the actual cognitive and
        emotional habits that separate designers who innovate from those who simply produce.
    </p>

    <p>
        What is striking is how much these two frameworks agree on, despite their very different styles.
        Both insist that understanding users is not optional—it is the foundation. Both emphasize iteration
        as essential rather than optional. And both acknowledge that designing for people is genuinely
        difficult, not because the methods are complicated, but because truly listening to and learning
        from other human beings requires patience, humility, and a willingness to be surprised by what you find.
    </p>

    <h2>Comparing the Two Frameworks: Process vs. Mindset</h2>

    <p>
        If you placed ISO 9241-210 and the IDEO Field Guide side by side, the most obvious difference is
        structural. The ISO standard gives you a clear, step-by-step loop. It tells you to plan, research,
        define requirements, design, evaluate, and repeat. It is engineered for organizations that need
        a repeatable, documentable process—something you can integrate into a waterfall or agile development
        cycle and point to as evidence that you took usability seriously (Wickens et al., 2004). There is
        real value in that kind of rigor. It keeps teams aligned. It creates checkpoints. It makes it harder
        to skip the parts of the process that feel slow or uncomfortable.
    </p>

    <p>
        IDEO, on the other hand, does not give you a rigid loop. It gives you a philosophy. The three phases—
        Inspiration, Ideation, Implementation—are deliberately broad, and the Field Guide is upfront about
        the fact that "human-centered design isn't a perfectly linear process, and each project invariably
        has its own contours and character" (IDEO, 2015). The emphasis falls less on following steps and more
        on cultivating a certain way of being in the design process. You should be curious. You should be
        comfortable not knowing the answer. You should prototype early and often, not because you expect your
        first attempt to work, but because the act of making something tangible is itself a form of learning.
    </p>

    <p>
        This difference matters because it reflects two different assumptions about where good design comes
        from. The ISO framework assumes that if you follow the right process with sufficient discipline, you
        will arrive at a usable product. IDEO's framework assumes that good design also requires a certain
        kind of creative courage—the willingness to sit with uncertainty, generate ideas that might fail,
        and treat failure not as a setback but as information. In practice, the strongest design teams draw
        on both. They use structured processes to stay organized and accountable, and they cultivate open,
        experimental mindsets to keep their work genuinely innovative.
    </p>

    <p>
        One area where the two frameworks converge with particular force is on the question of empathy.
        ISO 9241-210 requires that design be based on "a comprehensive understanding of the users, tasks,
        and working environment" and that users be directly involved in the design and development process
        (Wickens et al., 2004). IDEO goes further, framing empathy as one of the seven core mindsets of a
        human-centered designer. "Empathy is the capacity to step into other people's shoes, to understand
        their lives, and start to solve problems from their perspectives," the Field Guide states (IDEO, 2015).
        Empathy, in this view, is not just a research technique. It is a disposition—a way of approaching the
        world that makes better design possible.
    </p>

    <h2>Perception, Visual Hierarchy, and Why They Matter</h2>

    <p>
        Understanding how people think and perceive the world is not an abstract academic exercise. It is
        directly, practically relevant to every design decision you make. Human beings do not experience
        interfaces the way a computer does—pixel by pixel, element by element, in a neutral and objective
        way. We scan. We prioritize. We make rapid judgments based on visual cues, and we fill in gaps with
        assumptions drawn from past experience. If a design does not account for these tendencies, it will
        feel confusing, overwhelming, or simply wrong—even if every individual element is technically
        well-designed.
    </p>

    <p>
        Visual hierarchy is one of the most fundamental tools designers use to work with human perception
        rather than against it. It is the practice of organizing elements on a page or screen so that the
        most important information stands out and the least important information recedes. This is achieved
        through size, color, contrast, spacing, and position. A large, bold headline draws the eye before a
        smaller block of body text. A brightly colored button commands attention before a muted link. These
        are not arbitrary aesthetic choices. They are decisions rooted in how human visual processing
        actually works—how we distinguish figure from ground, how we group related elements, and how we
        direct our attention across a complex visual field (Wickens et al., 2004).
    </p>

    <p>
        When visual hierarchy is done well, the user barely notices it. The interface just feels intuitive.
        The right information is where they expect it to be. The next action they need to take is obvious.
        When it is done poorly, the opposite happens: the user has to work to figure out what is important,
        where to look, and what to do next. This friction—small as it might seem in any single moment—
        accumulates over time and degrades the entire experience.
    </p>

    <h2>The Four UX Laws and What They Teach Us</h2>

    <p>
        Several well-known principles from UX research help explain why certain design patterns feel so
        natural and others feel so clunky. Four of them are particularly relevant to the ideas we have
        been discussing.
    </p>

    <p>
        Jakob's Law is perhaps the most widely cited. It states that users prefer interfaces that work the
        way they already expect them to work, based on their experience with other products. This is not
        a suggestion—it is a description of how human cognition works. We build mental models of how things
        should behave, and when an interface violates those models, we experience friction and confusion.
        Jakob Nielsen, the usability researcher who popularized this principle, argues that designers should
        leverage existing conventions rather than constantly reinventing the wheel (Nielsen, 2000). This
        does not mean design should be boring or derivative. It means that novelty should be introduced
        thoughtfully, in ways that do not force users to abandon everything they already know.
    </p>

    <p>
        Fitts's Law comes from a different tradition—motor psychology rather than cognitive psychology—but
        its implications for design are just as significant. It states that the time it takes to move to a
        target is a function of the distance to that target and the size of the target itself. In practical
        terms, this means that important buttons and interactive elements should be large and easy to reach.
        A tiny, hard-to-click link buried in a corner of the screen is not just an aesthetic failure. It is
        a violation of a fundamental principle of human movement and interaction (Wickens et al., 2004).
        Good designers internalize Fitts's Law almost instinctively—they make the things users need to click
        bigger, closer, and more prominent.
    </p>

    <p>
        Hick's Law tells us that the time it takes a person to make a decision increases with the number of
        choices available to them. This is why overwhelming a user with too many options at once is almost
        always a mistake. A navigation menu with thirty items is harder to use than one with five, not
        because the user cannot read all thirty, but because the cognitive load of evaluating thirty options
        slows them down and increases the chance of error. Designers use Hick's Law to justify simplification—
        reducing menus, breaking complex tasks into smaller steps, and presenting information in manageable
        chunks rather than all at once (Wickens et al., 2004).
    </p>

    <p>
        Miller's Law rounds out the picture by reminding us that human working memory is limited. The classic
        formulation is that people can hold roughly seven items—plus or minus two—in working memory at any
        given time. This is why long lists, dense forms, and information-heavy screens so often feel
        overwhelming. It is not that users are not paying attention. It is that their cognitive capacity
        has a hard ceiling, and designs that exceed it will inevitably feel stressful and error-prone.
        Chunking information—grouping related items together so they can be processed as a single unit—is
        one of the most effective ways to work within this constraint (Wickens et al., 2004).
    </p>

    <p>
        Taken together, these four laws paint a clear picture: human beings are not infinitely flexible
        processors. They have habits, expectations, physical limitations, and cognitive constraints.
        Design that respects these realities will feel effortless. Design that ignores them will feel
        broken, no matter how visually polished it might be.
    </p>

    <h2>Bringing It All Together: Why "Human First" Is Not Just a Slogan</h2>

    <p>
        The phrase "human first" gets used a lot in design circles, and like any phrase that gets used a
        lot, it can start to feel hollow. But when you actually trace what it means—when you look at the
        ISO 9241-210 standard, when you read IDEO's Field Guide, when you study how perception and cognition
        shape every interaction a person has with a screen—it turns out to be one of the most consequential
        commitments a design team can make.
    </p>

    <p>
        Designing for people means accepting that your users are not a blank slate. They come with mental
        models built from years of experience. They have limited attention, limited working memory, and
        limited patience. They scan before they read. They make judgments in fractions of a second based on
        visual cues. They expect things to work the way other things they have used before have worked. And
        they will tell you, if you bother to ask, exactly what is confusing, frustrating, or broken about
        your product—but only if you create the conditions for them to do so.
    </p>

    <p>
        Both the ISO standard and IDEO's Field Guide are, at their heart, frameworks for creating those
        conditions. They are different in tone, structure, and emphasis, but they share a common conviction:
        that the best designs are not the ones that are most technically impressive or visually stunning.
        They are the ones that make the most sense to the people who use them. The ones that feel obvious.
        The ones that get out of the way and let people do what they came to do.
    </p>

    <p>
        That is what "human first" actually means. Not a buzzword. Not a marketing tagline. A design
        philosophy that, when practiced with discipline and genuine curiosity, produces work that lasts.
    </p>
       <footer class="article-footer">
                <h3>References</h3>
    <p>
        IDEO. (2015). <em>The field guide to human-centered design</em> (1st ed.). IDEO.org.
    </p>

    <p>
        Nielsen, J. (2000). Jakob's law. Nielsen Norman Group.
        https://www.nngroup.com/articles/jakobs-law-compat/
    </p>

    <p>
        Wickens, C. D., Hollands, J. G., Banbury, S., & Parasuraman, R. (2004).
        <em>Attentional resources</em> (2nd ed.). Routledge.
    </p>

    <p>
        Wickens, C. D., Hollands, J. G., Banbury, S., & Parasuraman, R. (2004).
        <em>Engineering psychology and human performance</em> (5th ed.). Routledge.
    </p>
                
            </footer>

    
        </article>
  
      
         


         <!-- Module 2.2 Blog Post Starts here -->
          <article>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Module 2.2: Consistency and Cognition</title>
</head>
<body>

<h1>Module 2.2 Assignment: Consistency and Cognition</h1>

<h2>Part 1: Cognitive Limitations in Digital Systems</h2>

<h3>How cognitive limitations affect human interaction with digital systems:</h3>

<p>Human cognition operates within specific constraints that fundamentally shape how we interact with technology. Our brains, while remarkably powerful, have inherent limitations in processing capacity, memory retention, and decision-making speed that designers must understand and accommodate.</p>

<p>One of the most significant cognitive limitations is our working memory capacity. Research shows that humans can typically hold only 5-7 discrete items in short-term memory simultaneously (Johnson, 2020). This limitation has profound implications for interface design. When a system presents too many options, requires users to remember information across multiple screens, or demands that users keep track of complex relationships between elements, it exceeds our cognitive capacity and leads to errors, confusion, and frustration.</p>

<p>Cognitive load - the total amount of mental effort being used in working memory - is another critical factor. Every element in an interface, every decision point, and every piece of information we must process adds to this load. When cognitive load becomes too high, our performance degrades rapidly. We make mistakes, overlook important information, and experience mental fatigue (Johnson, 2020). This is why cluttered interfaces with too many competing elements feel overwhelming and difficult to use.</p>

<p>Attention is also a limited resource. Humans cannot effectively multitask in the way we often believe we can. Instead, we rapidly switch attention between tasks, and each switch carries a cognitive cost. Interfaces that demand divided attention - such as those with distracting animations, auto-playing videos, or excessive notifications - fragment our focus and reduce our effectiveness. Johnson (2020) emphasizes that our attention is goal-driven and highly selective, meaning we focus on what we're looking for and often miss other information entirely.</p>

<p>Processing speed varies based on the type of cognitive task. Visual recognition is extremely fast - we can identify familiar patterns almost instantaneously. However, reading and comprehending text, performing calculations, or making complex decisions require significantly more time and mental resources (Johnson, 2020). Effective interface design leverages fast processes (like visual recognition) while minimizing demands on slower processes (like reading lengthy instructions or performing mental calculations).</p>

<p>Our cognitive systems also rely heavily on pattern recognition and expectation. When interfaces behave in unexpected ways or violate established conventions, users must engage in effortful, conscious processing rather than relying on automatic, learned responses. This dramatically increases cognitive load and slows down interaction.</p>

<h2>Part 2: Recognition vs. Recall in Interface Design</h2>

<p>Understanding the distinction between recognition and recall is fundamental to creating usable interfaces, as these two memory processes have vastly different cognitive demands.</p>

<p><strong>Recognition</strong> occurs when we encounter something and identify it as familiar based on previously stored information. It's the process of matching current sensory input with existing memories. Recognition is relatively effortless because the stimulus itself provides cues that trigger memory retrieval. When you see a friend's face in a crowd, you don't have to consciously search through your memories - recognition happens automatically and almost instantaneously (Johnson, 2020).</p>

<p><strong>Recall</strong>, in contrast, requires retrieving information from memory without external cues or prompts. It demands active searching through stored memories to find specific information. Trying to remember someone's phone number, recalling the name of a restaurant you visited last month, or remembering a specific command in software all require recall, which is cognitively demanding and prone to failure.</p>

<p><strong>In interface design, recognition is dramatically easier than recall.</strong> Johnson (2020) explains that recognition memory is far more robust than recall memory - we can recognize thousands of images we've seen before, even years later, but recalling specific details without cues is much more difficult. Studies show that recognition tasks can be performed 10-100 times faster than recall tasks, with significantly higher accuracy. This difference should fundamentally shape interface design decisions.</p>
   <div style="text-align:center;">
  <img src="q22.jpg" alt="https://www.linkedin.com/posts/thazin-win-723a75175_recognition-over-recall-what-is-recognition-activity-7192392223735402496-amF6">
</div>

<h3>Examples of recognition in interfaces:</h3>
<ul>
<li><strong>Menus and dropdown lists</strong> present all available options visually, allowing users to recognize the correct choice rather than having to remember it</li>
<li><strong>Icons and buttons</strong> provide visual representations that users can recognize, eliminating the need to remember commands or keyboard shortcuts</li>
<li><strong>Auto-complete features</strong> show suggestions as users type, supporting recognition of the desired option</li>
<li><strong>Breadcrumb navigation</strong> displays the current location and path visually, so users recognize where they are rather than having to remember</li>
<li><strong>Form fields with visible labels</strong> allow users to recognize what information is needed rather than recalling it</li>
</ul>

<h3>Examples of problematic recall demands:</h3>
<ul>
<li>Command-line interfaces that require memorizing exact syntax</li>
<li>Systems that require remembering information from one screen to use on another screen</li>
<li>Interfaces where settings or options are hidden with no visible indication they exist</li>
<li>Forms that don't show what format is expected (like date formats)</li>
</ul>

<p>Good designers systematically identify places where their interface demands recall and convert these to recognition-based interactions wherever possible. This might mean adding visible options instead of requiring typed commands, showing current state rather than making users remember it, or providing visual cues and labels that support recognition. As Johnson (2020) emphasizes, designers should "recognize recognition over recall" as a fundamental principle of human-centered design.</p>

<h2>Part 3: Understanding Tesler's Law</h2>

<h3>Definition and Core Concept:</h3>

<p>Tesler's Law, also known as the Law of Conservation of Complexity, states that for any system there exists a certain amount of inherent complexity that cannot be reduced or eliminated - it can only be transferred from one place to another. This complexity must be handled by either the user or by the designers and developers who create the system (Yablonski, n.d.).</p>

<p>Larry Tesler, a computer scientist at Xerox PARC, articulated this principle while working on developing interaction design standards in the 1980s. He recognized that while designers should strive to simplify user experiences, there's always a minimum threshold of complexity that cannot be removed because it's fundamental to what the system does.</p>

<h3>The Designer's Responsibility:</h3>

<p>The critical implication of Tesler's Law is that designers face a choice: should complexity burden the user, or should designers and developers absorb it during the design and development process? Tesler argued strongly that, whenever possible, complexity should be handled by those creating the system rather than passed along to users (Yablonski, n.d.).</p>

<p>His reasoning was compelling: if a million users each waste even one minute per day dealing with complexity that an engineer could have eliminated in a week of development work, you're essentially penalizing users to make the engineer's job easier. The cumulative cost to users far outweighs the one-time cost of addressing complexity in the design.</p>

<h3>Practical Examples:</h3>

<p>Consider email composition. You cannot send an email without specifying a sender and recipient - this is inherent, unavoidable complexity. Modern email clients handle this complexity by auto-populating the "from" field (since the system knows your email address) and providing auto-complete suggestions for recipients based on your contact list and previous emails (Yablonski, n.d.). The complexity hasn't disappeared - the system still needs to manage sender and recipient information - but it's been abstracted away from the user through thoughtful design and development work.</p>

<p>Another example is online shopping checkout. Purchasing items requires billing information, shipping details, and payment processing - all inherently complex. Amazon's "1-Click ordering" represents an extreme simplification where users can complete a purchase with a single click. All the complexity of payment processing, address verification, and order confirmation still exists, but it's been completely absorbed by the system through extensive upfront design and development. The user experiences simplicity while the system manages enormous complexity behind the scenes.</p>

<p>Progressive disclosure is a technique often used to manage complexity according to Tesler's Law. Advanced features and less-used options are hidden by default but remain accessible when needed. This shifts the complexity of deciding what to show and when from the user to the designer, who must carefully consider information hierarchy and usage patterns (Yablonski, n.d.).</p>

<h3>Important Caveat:</h3>

<p>While Tesler's Law encourages designers to absorb complexity, there's a balance to strike. Over-simplification can create abstraction that obscures important functionality or removes user control. The goal isn't to make everything completely automatic or to hide all complexity, but rather to carefully consider which complexity is necessary for users to manage directly and which can be handled by the system.</p>

<h2>Part 4: Translating Psychological Principles into Actionable Design Heuristics</h2>

<p>The gap between understanding psychological principles and actually applying them in design work can be substantial. Translating research-based insights into practical, actionable design guidance requires a systematic approach.</p>

<h3>The Translation Framework:</h3>

<p>The most effective approach involves creating a three-level framework that connects abstract psychological principles to concrete design decisions (Yablonski, n.d.):</p>

<p><strong>Level 1 - Psychological Principle/Law:</strong> This is the research-based observation about human cognition or behavior. Examples include Hick's Law (decision time increases with number of choices), Jakob's Law (users prefer interfaces that work like other sites they know), or the principle that humans have limited working memory capacity.</p>

<p><strong>Level 2 - Design Principle:</strong> This translates the psychological observation into a value or priority for your design team. Design principles are memorable, opinionated statements that capture what matters to your team. They provide direction without being overly prescriptive. Examples might include "clarity over abundance of choice" or "familiarity over novelty."</p>

<p><strong>Level 3 - Design Rules:</strong> These are specific, actionable guidelines that designers can follow in their daily work. Rules provide concrete constraints and direction. Examples might include "limit primary navigation to 5-7 items" or "use standard iconography from platform guidelines."</p>

<h3>Example Translation 1:</h3>
<ul>
<li><strong>Psychological Principle:</strong> Hick's Law - the time to make a decision increases with the number and complexity of choices</li>
<li><strong>Design Principle:</strong> "Clarity over abundance of choice"</li>
<li><strong>Design Rules:</strong>
<ul>
<li>Limit menu options to 3-5 items where possible</li>
<li>Use progressive disclosure to show advanced options only when needed</li>
<li>Provide clear defaults to reduce decision burden</li>
<li>Group related choices to simplify complex decisions</li>
</ul>
</li>
</ul>

<h3>Example Translation 2:</h3>
<ul>
<li><strong>Psychological Principle:</strong> Jakob's Law - users spend most of their time on other sites and prefer your site to work the same way</li>
<li><strong>Design Principle:</strong> "Familiarity over novelty"</li>
<li><strong>Design Rules:</strong>
<ul>
<li>Use common design patterns (hamburger menu, shopping cart icon, etc.)</li>
<li>Follow platform conventions for your operating system</li>
<li>Place navigation, search, and key functions where users expect them</li>
<li>Avoid unnecessary animations or unconventional interactions</li>
</ul>
</li>
</ul>

<h3>Example Translation 3:</h3>
<ul>
<li><strong>Psychological Principle:</strong> Limited working memory capacity (5-7 items) (Johnson, 2020)</li>
<li><strong>Design Principle:</strong> "Support memory, don't demand it"</li>
<li><strong>Design Rules:</strong>
<ul>
<li>Keep form fields visible on screen rather than paginating</li>
<li>Show order summary during checkout process</li>
<li>Use persistent navigation so users always know where they are</li>
<li>Provide contextual help rather than requiring users to remember instructions</li>
</ul>
</li>
</ul>

<p>This framework ensures that design decisions are grounded in psychological reality while remaining practical and applicable to everyday design work. By establishing clear connections between psychological research and design practice, teams can make informed decisions that genuinely improve user experience rather than relying on assumptions or trends (Norman, 2013).</p>

<h2>Part 5: Design Principles for Memory Retention and Reducing Cognitive Load</h2>

<p>Creating interfaces that work with human memory and minimize cognitive burden requires understanding and applying several interconnected design principles.</p>

<h3>Supporting Memory Retention:</h3>

<p><strong>Chunking:</strong> Breaking information into meaningful groups dramatically improves our ability to remember it. Phone numbers are chunked (555-123-4567 rather than 5551234567), making them easier to remember. In interfaces, group related functions together, organize content into logical sections, and use visual grouping (whitespace, borders, color) to chunk information. This principle capitalizes on how our working memory naturally organizes information into manageable units (Miller, 1956). Johnson (2020) explains that chunking works because it reduces the number of discrete items we must hold in working memory, even though the total amount of information remains the same.</p>

<p><strong>Consistency:</strong> When interfaces use consistent patterns, users only need to learn something once and can apply that knowledge throughout the system. Consistent placement of navigation, consistent button styling, and consistent interaction patterns all reduce memory load because users build reliable mental models. Johnson (2020) emphasizes that consistency reduces the learning burden and helps users develop accurate expectations about how the system will behave. This aligns with Jakob's Law, which recognizes that users transfer expectations from familiar sites to new ones.</p>

<p><strong>Progressive Disclosure:</strong> Rather than overwhelming users with all available information and options at once, progressive disclosure reveals information as it becomes relevant. Dropdown menus, accordions, and collapsible sections all embody this principle. This reduces the amount of information users must process and remember at any given moment (Yablonski, n.d.). By showing only what's necessary for the current task, progressive disclosure prevents cognitive overload.</p>

<p><strong>Recognition Over Recall:</strong> As discussed earlier, supporting recognition rather than demanding recall dramatically reduces memory load. Visible options, clear labels, and contextual cues all support recognition. This principle is fundamental because recognition requires minimal cognitive effort compared to the demanding process of recall (Johnson, 2020).</p>

<p><strong>Visual Hierarchy:</strong> Making important information visually prominent through size, color, position, or contrast helps users quickly identify and remember what matters most. Clear hierarchy reduces the mental effort required to scan and process information, allowing users to focus their limited attention on what's truly important. Johnson (2020) notes that our visual system processes information in parallel and can quickly detect patterns and differences, making visual hierarchy an efficient way to guide attention.</p>

<div style="text-align:center;">
  <img src="are.png" alt="https://www.linkedin.com/pulse/cognitive-design-five-visualisation-principles-compelling-taher-miah/">
</div>

<h3>Reducing Cognitive Load:</h3>

<p><strong>Minimize Decision Points:</strong> Every choice requires cognitive effort. Reduce unnecessary decisions by providing smart defaults, eliminating redundant options, and streamlining workflows. When choices are necessary, keep the number manageable. Research on decision fatigue demonstrates that our capacity for making good decisions depletes with each choice we make (Baumeister & Tierney, 2011). Johnson (2020) explains that decision-making consumes working memory resources and mental energy, so minimizing unnecessary decisions preserves users' cognitive capacity for important tasks.</p>

<p><strong>Provide Clear Feedback:</strong> Immediate, clear feedback about the results of actions reduces uncertainty and the cognitive load of wondering whether something worked. Users don't have to remember what they just did or worry about system state. This principle of visibility and feedback is fundamental to creating systems that users can understand and control. Johnson (2020) emphasizes that feedback should be immediate and clearly connected to the action that triggered it.</p>

<p><strong>Use Familiar Patterns:</strong> Leveraging existing mental models and conventions means users can rely on prior knowledge rather than learning something new. This includes using standard icons (trash can for delete, magnifying glass for search), following platform conventions, and adhering to established interaction patterns. When systems behave as users expect, cognitive load decreases dramatically (Norman, 2013). Johnson (2020) notes that familiarity allows users to rely on automatic, practiced responses rather than conscious, effortful processing.</p>

<p><strong>Reduce Visual Clutter:</strong> Every element in an interface competes for attention and processing resources. Ruthlessly prioritize what's essential and eliminate or de-emphasize everything else. White space isn't wasted space - it gives the brain breathing room and makes interfaces feel more manageable. Johnson (2020) explains that visual clutter increases the time required to find information and makes it harder to distinguish important elements from unimportant ones.</p>

<p><strong>Provide Contextual Guidance:</strong> Rather than requiring users to remember instructions or hunt for help documentation, embed guidance contextually where it's needed. Tooltips, inline help text, and placeholder text in form fields all provide support exactly when and where it's useful. This addresses the Paradox of the Active User - the observation that people don't read manuals but instead start using software immediately (Yablonski, n.d.).</p>

<h2>Part 6: Cognition-Based Design Principles</h2>

<p>Synthesizing these insights yields several fundamental cognition-based design principles that should guide interface design:</p>

<h3>1. Designers Should Bear Complexity, Not Users</h3>

<p>Following Tesler's Law, this principle holds that whenever there's a choice about who manages complexity, it should be the design and development team. This doesn't mean hiding all complexity or removing user control, but rather thoughtfully considering where complexity lives and ensuring users aren't burdened unnecessarily. The goal is to create interfaces that feel simple and intuitive while the system handles the complexity behind the scenes.</p>

<h3>2. Design for Actual Human Behavior, Not Idealized Behavior</h3>

<p>The Paradox of the Active User tells us that people don't read manuals - they start using software immediately, learning by doing (Yablonski, n.d.). Design must accommodate this reality by providing contextual guidance, making functionality discoverable, and creating interfaces that can be learned through exploration rather than study. Johnson (2020) emphasizes that designers must account for how people actually behave, including their tendency to satisfice (choose the first acceptable option) rather than optimize (search for the best option).</p>
<div style="text-align:center;">
  <img src="q33.jpg" alt="https://www.learningeverest.com/application-of-cognitive-constructivism-in-learning-design/">
</div>

<h3>3. Support Mental Models and Leverage Existing Knowledge</h3>

<p>Users approach new systems with existing mental models built from prior experience. Effective design aligns with these models rather than fighting them. Use familiar patterns, follow conventions, and make your system behave the way users expect based on their experience with other systems. When you must introduce new patterns, make them learnable and consistent throughout the system (Norman, 2013). Johnson (2020) explains that when interfaces violate users' mental models, they must engage in conscious, effortful learning, which increases cognitive load and error rates.</p>

<h3>4. Make System State and Available Actions Visible</h3>

<p>Visibility reduces memory load and uncertainty. Users should be able to see where they are, what they can do, and what will happen as a result of their actions. Hidden functionality might as well not exist for most users. This principle of visibility extends to showing system status, making navigation clear, and ensuring that interactive elements look interactive (Johnson, 2020).</p>

<h3>5. Favor Recognition Over Recall Systematically</h3>

<p>This principle should be applied comprehensively throughout interface design. Wherever users would need to remember something - commands, options, previous inputs, system state - consider how design can instead support recognition. This might mean showing options in menus rather than requiring typed commands, displaying current selections rather than hiding them, or providing visual previews of what actions will do (Johnson, 2020).</p>

<h3>6. Respect Cognitive Limitations</h3>

<p>Design with awareness of working memory constraints, attention limitations, and processing capacity. This means limiting choices, reducing clutter, minimizing distractions, and breaking complex tasks into manageable steps. Recognize that users have finite mental resources and that every demand you place on those resources has a cost. The best interfaces are those that feel effortless because they work within human cognitive constraints (Johnson, 2020).</p>

<h3>7. Consistency Amplifies Learnability</h3>

<p>Consistent interfaces are learnable interfaces. When patterns, terminology, and interactions remain consistent, users build reliable mental models that reduce cognitive load over time. What users learn in one part of the system should apply throughout. Inconsistency forces users to constantly relearn and adapt, wasting cognitive resources that could be better spent on accomplishing their goals (Johnson, 2020).</p>

<h3>Practical Application:</h3>

<p>When making design decisions, continually ask: "Am I making users work harder mentally, or am I solving this complexity in the design?" The answer should consistently push complexity toward the design solution. Consider whether users need to remember information, make complex decisions, or learn new patterns - and look for opportunities to reduce these cognitive demands through thoughtful design.</p>

<p>The most successful interfaces feel almost invisible because they work with human cognition rather than against it. They anticipate needs, guide without instructing, and make complex tasks feel simple. This is achieved not by magic, but by systematically applying principles grounded in how human minds actually work.</p>

<p>Understanding cognition-based design principles allows designers to create experiences that feel intuitive and effortless. By respecting human cognitive limitations, supporting memory through recognition rather than recall, managing complexity on behalf of users, and maintaining consistency throughout systems, designers can create interfaces that enhance rather than hinder human capability. These principles aren't optional considerations - they're fundamental requirements for creating technology that truly serves human needs.</p>

<footer class="article-footer">
<h3>References</h3>

<p>Baumeister, R. F., & Tierney, J. (2011). <em>Willpower: Rediscovering the greatest human strength</em>. Penguin Press.</p>

<p>Johnson, J. (2020). <em>Designing with the mind in mind: Simple guide to understanding user interface design guidelines</em> (3rd ed.). Morgan Kaufmann.</p>

<p>Miller, G. A. (1956). The magical number seven, plus or minus two: Some limits on our capacity for processing information. <em>Psychological Review, 63</em>(2), 81–97. https://doi.org/10.1037/h0043158</p>

<p>Norman, D. A. (2013). <em>The design of everyday things</em> (Revised and expanded edition). Basic Books.</p>

<p>Yablonski, J. (n.d.). <em>Laws of UX</em>. https://lawsofux.com</p>

</footer>

</body>
</html>
</article>


         <!-- Module 2.3 Blog Post Starts here -->

         <!-- Module 3.1 Blog Post Starts here -->

         <!-- Module 3.2 Blog Post Starts here -->

      </main>
<!-- Page Footer  -->
      <footer class="page-footer">
       <p>Copyright &copy; 2025</p>
      </footer>
   </body>
</html>