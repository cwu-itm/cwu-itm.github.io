<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <link href="css/styles.css" rel="stylesheet">
      <title>Arya Ganjikunta | Portfolio</title>
   </head>
   <body>
<!-- Page Header  -->
      <header class="page-header">
         <h1>Portfolio</h1>
      </header>
<!-- Main Content Area  -->
      <main>
         <!-- Module 1.1 Blog Post Starts Here-->
         <article>
            <header class="article-header">
               <h2>Module 1.1 Blog Post</h2>
            </header>
            <h3>What Systems thinking means to me</h3>
            <p>Before I read through a few sources, systems thinking to me just meant a collection of parts working together just as how I see any other physical system in the world. If something breaks, then you fix it and move on. This mindset of thinking does work but only with simple tools, when it comes to larger digital systems it isn’t as simple as this. Systems thinking feels less like repairing parts and more about understanding relationships between different aspects. Instead of seeing why something is failing it’s important to see what interactions led to this problem. Systems thinking is the mindset of seeing any product, organization, or digital environment as more of a network of connected elements that usually influence each other over time. Instead of just isolating problems it forces you to look at patterns, feedback loops, delays, and unintentional consequences. It’s more focused on the structure of the elements instead of the detail. Holt (2019) describes systems thinking as recognizing that systems behave differently than their individual components would make you think. This is important because when you change one part of the system it affects the others which can lead to unexpected results from a single change. For example, a bug fix or a feature update could solve one issue in one section but create three more issues in another section.</p>
            <h3>How Systems Thinking is Different from Linear Thinking</h3>
            <p>Linear thinking is usually when you see things in the form of steps. Like if you do A, B, and C then you expect an outcome of D. This way of thinking assumes that causes and effects are completely predictable and direct. In the real world and especially in more software focused ecosystems this would not work since things do not work that way. Linear thinking works well for tasks like following a recipe or assembling furniture. However digital systems aren’t recipes they are closer to an ecosystem. A small change of code in one microservice could affect dozens of dependent services which could change performance, change security exposure, or affect user experience all at once. Meadows (2008) explains that linear thinking makes us treat problems as static, but systems are dynamic. They evolve, respond, and learn. A system that works perfectly today could fail tomorrow because of a new workload, new users, or new constraints. For example, when a company introduces a new feature that’s meant to improve user engagement, the linear expectation here is that the engagement increases. The way systems thinking approaches this is they see that it might increase server loads, introduce new attack opportunities, frustrate power users, or even reduce performance for people on certain devices. These sort of effects aren’t that obvious to most people unless you have trained yourself to look for the interactions instead of the outcomes. </p>
            <h3>Why Digital Systems Demand Systems Thinking</h3>
            <p>Our modern digital systems don’t require systems thinking just because they’re big, it’s also the fact that they are interdependent. API’s talk to databases which gives information to analytics platforms, which drive machine learning models, which influence what business decisions are made. No part really exists alone or isolated from everything else. INCOSE (2022) says that complexity grows not from size but from connectivity. The more connected a system becomes the more unpredictable it is. This unpredictability is what can cause outages, data leaks, or some sort of system failure. This is why systems thinking isn’t really optional within the Information Technology industry anymore. It’s part of the base knowledge needed to succeed. Without it engineers can build solutions that are optimized locally but would be globally broken. You might improve one metric which sneakily destroys another more important one. </p>
            <h3>What Model-Based Systems Engineering Adds</h3>
            <p>Systems thinking changes more of how you see the problem. Model-Based Systems Engineering changes how you design the solution. MBSE replaces static documentation with living models that show requirements, behavior, structure, and constraints in a more formal way. Instead of writing long Word documents that take a long time to read, these engineers build interconnected diagrams that update automatically as the system evolves. (Holt, 2019) The real power of MBSE isn’t for the convenience it’s the alignment of everything. Every requirement is connected to a design element, every design element is connected to a test case. When something changes the model shows the ripple effect immediately. </p>
            <h3>Why Traditional Documentation Fails at Scale</h3>
            <p>In traditional development environments documentation is more fragmented. One team writes requirements while another designs the system. Another person tests it. Over time these documents end up drifting apart. This means the system changes but the documentation doesn’t. This leads to situations where nobody fully understands the system anymore. New engineers usually kind of go off instinct or more tribal knowledge. Bugs get fixed by habit instead of insight. MBSE gets rid of this problem by making the model the single source of truth. According to INCOSE (2022), organizations using MBSE report fewer integration errors, better risk management, and faster onboarding for new engineers. That is not because MBSE makes people smarter it just makes the complexity more visible. </p>
            <h3>MBSE as a Tool for Thinking, Not Just Designing</h3>
            <p>One of the most important things I learned from this reading is that MBSE is not just a technical framework its also a thinking framework. It forces you to think in relationships not in features. Instead of wondering how we are supposed to build a model it’s more what role does this module play in the system. You stop optimizing for speed alone and start optimizing for resilience and more long term sustainability overall.</p>
            <h3>Why MBSE Fits Modern Digital Environments</h3>
            <p>Digital environments change constantly such as new technology that gets created, security threats that evolve, and user expectations that change. MBSE gives you a way to adapt without losing control of the system. Meadows (2008) says that systems fail most often when designers misunderstand leverage points which are the small changes that can have a big effect. MBSE emphasizes these leverage points by showing how tight knit parts of the system are. This matters especially in cloud based architectures where services auto scale and communicate and evolve independently. Without a model no one can reliably predict how the system will act under stress.</p>
            <h3>What this means for me as a student</h3>
            <p>Before this assignment I thought systems engineering was mostly about the process but now I know it’s also about the mindset. Systems thinking isn’t really just limited to the office or work environment, I realized I can use it in my daily life and actually use it as an advantage. It has made me more aware of being careful with quick fixes and more curious about the system part of it, like what is the root cause. Instead of asking why something is broken I would ask what relationships made it break. MBSE is basically that mindset put into a physical form that allows you to take action on it. </p>
            <h3>Conclusion</h3>
            <p>Systems thinking is centered around seeing beyond just seeing the parts and actually looking at patterns. It doesn’t allow you to think in assumptions but actually observe and confront the reality of the complexity. MBSE gives us the structure to use that understanding in the real world. Together they are the foundation of systems engineering. Without them digital systems grow faster than our ability to control them. With them we can design environments that can evolve without collapsing or having issues.</p3>
            <footer class="article-footer">
  <h3>Sources</h3>
  <ol>
    <li>
      Holt, J. (2023). <em>Systems engineering demystified: Apply modern, model-based systems engineering techniques to build complex systems</em> (2nd ed.). Packt Publishing.
      <a href="https://searchlib.cwu.edu/discovery/fulldisplay?docid=cdi_globaltitleindex_catalog_406921727&context=PC&vid=01ALLIANCE_CWU:CWU&lang=en&search_scope=CWU_MORE&adaptor=Primo%20Central&tab=CWU_MORE&query=any,contains,Systems%20engineering%20demystified">https://searchlib.cwu.edu/...</a>
    </li>
    <li>
      Olwell, D. H., Henry, D., Pyster, A., Hutchison, N., Enck, S., &amp; Anthony, J. F. (2013). Analysis of the references from the Guide to the Systems Engineering Body of Knowledge (SEBoK). <em>Procedia Computer Science, 16</em>, 1000–1006. https://doi.org/10.1016/j.procs.2013.01.105
    </li>
    <li>
      Meadows, D. H., &amp; Wright, D. (2008). <em>Thinking in systems: A primer</em>. Chelsea Green Publishing.
      <a href="https://adams.marmot.org/Record/.b33648414/Cite">https://adams.marmot.org/...</a>
    </li>
  </ol>
</footer>


         <!-- Module 1.2 Blog Post Starts here -->
<article>
            <header class="article-header">
               <h2>Module 1.2 Blog Post</h2>
            </header>
            <h3>The Systems Development Lifecycle</h3>
            <p>The systems development life cycle refers to the structured stages a system goes through from the initial concept until the end of the system. Most models have phases like planning, requirements analysis, design, implementation, testing, deployment, operation, maintenance, and disposal (Holt, 2023). This framework becomes more useful once you realize that systems don’t just exist in one single moment. They usually exist over time and decisions that were made early on in making it could continue to affect the system even a long time after it’s deployed. What stands out to me is that the SDLC isn’t as focused on following a fixed set of steps its more about trying to maintain control over risks and changes. </p>
            <h3>The Linear Life Cycle Model</h3>
            <p>The linear life cycle model which is also known as the waterfall model follows a sequence of events that progress from one phase to the next. They usually move from requirements to design, implementation to testing, and deployment without really overlapping steps. (Pressman, 2010). This shows that each phase is expected to be completed before going on to the next. One of the main advantages of this model is the clarity. Planning and scheduling are more straightforward with this model because each phase is very clearly defined. This would be more useful in an environment where there are formal approval processes or there are heavy regulations you have to follow. The model is also good for traceability because all the requirements are directly linked to the different parts of the system. One disadvantage is that this model depends on whether or not you are able to define all the requirements fully at the beginning of the project. This is usually not how it works in the real world. The users are always changing and some technical problems might not be evident until the development is started. When errors happen early in the process they are usually discovered late which increases the cost and complexity of fixing them (Boehm, 1988). This makes the linear model not that good for more complicated systems or systems that are constantly evolving. </p>
            <h3>The Iterative Life Cycle Model</h3>
            <p>The iterative life cycle has a different approach than the linear one. This model repeats the development process multiple times where each cycle makes a more refined version of the system (Holt, 2023). This model doesn’t have all the requirements up front it actually learns it through the development cycles. What this means for the model is that it accepts the uncertainty instead of trying to eliminate it. The earlier versions of the system will test assumptions and gather feedback, then identify problems that weren’t there during the initial planning. Then with that information you can incorporate solutions into different iterations which leads to improvement over time. One advantage of this model is that it lowers the chance of building a system that doesn’t meet user needs even if it technically works for the documented requirements. This is due to the fact that feedback is constantly incorporated into the way the system changes and evolves and the fact that it responds to real usage. However, there are still some issues that can come with this model. The main potential issue is regarding scope control and scheduling. If the model doesn’t have clear boundaries then it can grow way farther than what was intended originally. This means that the timelines can become difficult to predict. So to prevent this the model needs to be managed carefully so that it doesn’t become inconsistent from it’s growth. </p>
            <h3>Incremental Life Cycle Model</h3>
            <p>The incremental life cycle model is more focused on delivering the system in a series of functional increments with each increment adding new parts or capabilities to the system (Pressman, 2010). Each release is meant to be fully operational and stable so not really a prototype. This type of approach allows for the stakeholders of the project to test parts of the system or fully use them before the whole thing is complete. Because of this the feedback value is given to the system quicker and its feedback would be based on real use instead of hypothetical scenarios. This makes it so the risk is lesser since it’s more distributed instead of being one single deployment with a huge chance of risk. Again though, there are a few disadvantages. It’s not that big of a disadvantage but the whole model depends on how strong the architectural planning for it is. If some of the early increments have bad structure they can end up creating long term maintenance problems or limit the model. Managing all the individual dependencies among the increments would also be an issue because sometimes some of the features might rely on something that hasn’t been created or implemented yet. For example if one increment has two dependencies in the increment that’s supposed to come after it then it wouldn’t work properly without them.</p>
            <h3>Why Lifecycle Thinking is Important</h3>
            <p>The mindset of lifecycle thinking isn’t necessarily too focused on building the system but it’s more focused on sustaining it. Instead of just using an evaluation from the success of deploying the system this mindset would measure the success of the system based on how the system will perform and evolve over time. One reason for this is that a lot of early designing decisions can end up having long term consequences. Architectural choices, technology sections, and interface standards shape how easily a system can be updated, scaled, or integrated with other systems in the future (Holt, 2023). The system could work perfectly at the beginning but that could become the reason that it becomes difficult or expensive to maintain it down the line. A life cycle thinking cycle also helps with sustainability. Most of these systems end up using quite a few resources throughout their lifetime such as energy use or maintenance costs which all contribute to the system’s overall impact. Considering these factors across the entire life cycle allows engineers to make decisions that reduce long term costs and environmental impact (Boehm, 1988). Another thing that is very clear here is that change is not something you can avoid. Its either that technology is constantly evolving or user needs constantly change with a few other factors. The life cycle thinking mindset sees this as a completely normal part of the process instead of an exception. This is kind of similar to iterative and incremental models in terms of how they use adaptation as a big part of the model instead of seeing the changes that happen as a disruption.</p>
            <h3>Choosing Between Life Cycle Models</h3>
            <p>The way I see it there isn’t really one life cycle model that works the best for all situations. This is because each model is based on different assumptions regarding stability and risk. The linear model would be the best when the requirements are stable and risks are very well defined, especially when formal documentation is needed. The iterative model is better for environments where the requirements aren’t as clear so you have to use learning to understand the whole process. The incremental model is better when early delivery matters and the system can be built in different independent but functional parts. In the real world many projects combine different aspects of each model. The main thing that matters more than just sticking to one model because it has worked before is that the approach makes sense with what the system’s context and constraints are.</p>
            <h3>Conclusion</h3>
            <p>The systems development life cycle gives a good structured way to understand how systems are created and used. The three different models such as linear, iterative, and incremental models give different ways of organizing this process each with its own strengths and weaknesses. The need for different models is due to the fact that nowadays most systems aren’t static. They tend to change over time and the way you develop the system has to account for that. The life cycle thinking mindset teaches engineers to not worry as much about the short term delivery and more about the long term where adaptability and sustainability are part of the core aspects of the system. </p>
            
            <footer class="article-footer">
  <h3>Sources</h3>
  <ol>
    <li>
      Holt, J. (2023). <em>Systems engineering demystified: Apply modern, model-based systems engineering techniques to build complex systems</em> (2nd ed.). Packt Publishing.
      <a href="https://searchlib.cwu.edu/discovery/fulldisplay?docid=cdi_globaltitleindex_catalog_406921727&context=PC&vid=01ALLIANCE_CWU:CWU&lang=en&search_scope=CWU_MORE&adaptor=Primo%20Central&tab=CWU_MORE&query=any,contains,Systems%20engineering%20demystified">https://searchlib.cwu.edu/...</a>
    </li>
    <li>
      Boehm, B. W. (1988). A spiral model of software development and enhancement. Computer (Long Beach, Calif.), 21(5), 61–72. https://www.cse.msu.edu/~cse435/Homework/HW3/boehm.pdf
    </li>
    <li>
      Pressman, R. S. (2010).  <em>Software engineering : a practitioner’s approach (Seventh edition, alternate edition.). McGraw-Hill Higher Education. </em> https://drive.google.com/file/d/0B4FvADGfA7T8S3lCNE1IZlpQc1E/view?resourcekey=0-O9gbq1v6Nhr4ZisAbOJ_hg
    </li>
  </ol>
</footer>
         <!-- Module 2.1 Blog Post Starts here -->
<article>
            <header class="article-header">
               <h2>Module 2.1 Blog Post</h2>
            </header>
            <h3>What it Means to Design for People</h3>
            <p>When I first thought about what it means to design for people I thought it would be a more simple explanation such as making a certain button bigger and making another smaller. This was not the case. There’s another layer to it underneath which has to do with more specific criteria. It focuses on things like human perception and how we pay attention, also how we interpret what we see. It sounds like common sense like having one button be bigger makes people click on it, or if a form is simple then people will fill it out. The more I read about human centered design the more I realized its more based on actual science in terms of psychology instead of just picking what seems logical. </p>
            <h3>Understanding Users Through Perception and Attention</h3>
            <p>Human centered design is very focused on putting the people who will use your product at the center of every decision. The ISO 9241-210 framework defines HCD as a process that ensures the system or product is usable, useful, and desirable for the people interacting with it (ISO, 2021). What stands out here to me is how formal and structured the ISO approach is. It’s very process oriented which means there’s a lot of steps like planning, research, testing, etc. </p>
            <h3>ISO vs. IDEO</h3>
            <p>IDEO’s field guide is kind of similar but is still a little different. It’s mainly more focused on empathy. This involves making observations of users, trying to understand their behavior and frustration, and iterating rapidly based on what you see (IDEO, 2015). Reading both of them together I noticed that they are actually complimentary for each other. One of them gives you a structure and checklist while the other gives you human insight and information. </p>
            <h3>Applying UX Laws to Real Design Choices</h3>
            <p>Thinking about perception and attention in this context made me realize how much the design of products we use are influenced by how our minds work. Now I notice it everywhere like if I open an app and I know where to look immediately, or when a website feels confusing even though the information is technically there. Those are both examples of perception. For example Jakob’s law says that users spend most of their time on other sites which means they prefer familiar patterns (Yablonski, 2024). This essentially means that our brains are lazy but in a good way since we tend to rely on patterns that we have already seen or know about. When a website does something unexpected it can throw people off even if it works as it’s supposed to. That’s why designers who don’t take Jakob’s law in as a factor have the risk of making the users frustrated regardless of whether the website is usable or not. </p>
            <h3>Fitts’s Law and Physical Interaction</h3>
            <p>The next law is Fitt’s Law. This law focuses more on how quickly and accurately people can move to a target depending on its size and distance (Yablonski, 2024). In general I never thought about this until I started noticing certain small buttons in some apps that are much harder to click on or pretty much impossible to tap because they are too small. This can seem like a minor inconvenience but really it can ruin a whole user experience by itself. Even something as simple as putting a “Submit” button in a spot that’s not where the eye naturally ends when making a form can make people hesitate and even make mistakes. Designers who have a true comprehensive understanding of Fitt’s law are able to make that design connection feel almost invisible to the user in a way that the user wouldn’t even have to think about the design aspect of the application. If the application isn’t straightforward and using Fitt’s law, it can make them ask “Who in their right mind made this website?” but that whole thought process is eliminated when it is applied correctly. </p>
            <h3>Hick’s Law and Decision Fatigue</h3>
            <p>Another law is Hick’s Law. It says that the more choices we give people longer it takes for them to make a decision (Yablonski, 2024). This isn’t as obvious of an idea to think of since making a decision to most people doesn’t seem like anything taxing or hard to do. However what people don’t realize is that there is something known as decision fatigue which most apps account for already. This fatigue is factored into most design elements like choosing what to watch on a streaming platform or deciding which settings to configure in an app. When the users are overloaded with decisions they don’t make better choices, instead they end up procrastinating and making random selections. Using Hick’s law means that the designers will make the interface subtly guide the users to make decisions without overwhelming them. It’s supposed to be a good balance depending on the type of application it is. </p>
            <h3>Miller’s Law and Working Memory</h3>
            <p>Miller’s law is similar to this but more focused on the memory of the users. Humans can hold about seven items in working memory at a time, give or take one or two (Yablonski, 2024). That made me think more about when I look at menus in a restaurant and this made sense. When I look at a menu I tend to have trouble deciding when the menu is either massive or there are too many items on one page. Another great example is on the DoorDash application. I constantly scroll through DoorDash to find what I want to eat for the night, but if too many things are open I can struggle for a while on deciding what I want. This can lead to me trying to decide until most of the restaurants end up closing and I’m forced to go to whatever is open. Sectioning the pieces of information is important for actual usability since it’s not something that’s meant to be just “nice to have”. </p>
            <h3>Visual Hierarchy and Attention</h3>
            <p>Visual hierarchy is also very important when it comes to designing. Visual hierarchy is referring to how designers use size, spacing, positioning, etc. to basically communicate with human perception. When something is bigger or bold and more colorful our attention naturally goes towards it. This shows how our brains prioritize certain information over other information just based on the design. Designers basically exploit this to guide the users through tasks, but this can also accidentally mislead or frustrate them. For example if a primary action button looks smaller or less prominent than a secondary action button then users could completely miss the primary action button, even if the text is clear. Essentially visual hierarchy establishes a conversation with the user and if the user can’t understand the conversation then it goes down the drain.</p>
            <h3>Real-Life Application Examples</h3>
            <p>I personally found it helpful to connect this to real life examples. For example, lets say there’s a login form that was made for users. If the “Login” button was tiny and the “Forgot Password?” link is huge and brightly colored, then I would most likely end up clicking the wrong thing. This is Fitt’s law in real time combined with visual hierarchy. Now think about a streaming app that has ever genre, subgenre, and even another level down, that would be Hick’s law. My first instinct here would be to scroll past it or randomly click something because there are way too many options for me to properly evaluate what I want to watch. A moment like this can even be frustrating for me because navigating through all of those choices would make me feel a little overwhelmed. </p>
            <h3>Balancing Structure and Empathy in Design</h3>
            <p>Back to the ISO 9241-210 versus IDEO’s field guide, I noticed that both of the frameworks are useful but in different ways. ISO has a kind of checklist mentality that is meant to help avoid missing any basic or fundamental usability issues. It’s structured and logical so it’s more a formal approach. IDEO’s field guide shows me that I need to slow down and try to empathize with the user. You have to know what the user is feeling, like what do they notice first, what frustrates them, etc. This one is more on the human side of things instead of being as formal. The balance between these two perspectives is what makes the design user centered. I don’t think that you can really have one without the other and be successful.</p>
            <h3>Reflecting on My Own User Behavior</h3>
            <p>When I was reflecting all of this onto myself, I tried to think of my own tendencies when I’m an unknowing user. Of course this might not be accurate due to the fact that I am currently thinking in this perspective for this assignment but I will take a guess. I realized that when I’m frustrated I tend to blame myself first by thinking “I must not be doing this right”. A good designer can actually predict that. They know that the user would get annoyed if the application isn’t easy to use on top of being functional. </p>
            <h3>Broader Human-Centered Design Principles</h3>
            <p>I also wanted to talk about some broader HCD principles outside of the formal frameworks. The Interaction Design Foundation says that HCD is iterative and involves users in every stage, from initial observation to testing prototypes (Interaction Design Foundation, n.d.). I personally like this idea because it supports something that I already believe. This is that nobody understands the users better than the users themselves. Even the most experienced designer would not be able to anticipate every single thing. </p>
            <h3>Conclusion</h3>
            <p>Summing everything I learned from this assignment up, I now know that perception, empathy, and attention are the active factors that shape every decision that a designer makes. This can be deciding how big a button is, or how many options to show the user, or how to visually prioritize certain content. All of these affect how usable the application is for the users. It is also important to note that usability isn’t the only important factor in designing the application. You have to be able to create an experience that allows for the user to navigate through effortlessly and intuitively. That is the layer that can be more difficult for designers.</p>
            <footer class="article-footer">
  <h3>Sources</h3>
  <ol>
    <li>
      Yablonski, J. (2024) <em>Laws of UX</em> (2nd ed.). O'Reilly Media, Inc.
      <a href="https://cwu.vitalsource.com/books/9781098146924">https://searchlib.cwu.edu/...</a>
    </li>
    <li>
      Ergonomics of human-system interaction: An ergonomic approach to accessibility within the ISO 9241 series = Approche ergonomique de l’accessibilité dans la série / Ergonomie de l’interaction homme-système: Vol. ISO 9241-20:2021 (Second edition.). (2021). International Organization for Standardization. https://searchlib.cwu.edu/discovery/fulldisplay?docid=cdi_globaltitleindex_catalog_418774908&context=PC&vid=01ALLIANCE_CWU:CWU&lang=en&search_scope=CWU_MORE&adaptor=Primo%20Central&tab=CWU_MORE&query=any,contains,Ergonomics%20of%20human-system%20interaction&offset=0 
    </li>
    <li>
      The field guide to human-centered design: design kit (1st edition.). (2015). IDEO.  </em> https://searchlib.cwu.edu/discovery/fulldisplay?docid=cdi_globaltitleindex_catalog_31235996&context=PC&vid=01ALLIANCE_CWU:CWU&lang=en&search_scope=CWU_MORE&adaptor=Primo%20Central&tab=CWU_MORE&query=any,contains,The%20field%20guide%20to%20human-centered%20design&offset=0
    </li>
    <li>
      Interaction Design Foundation. (n.d.). Human-centered design (HCD). https://www.interaction-design.org/literature/topics/human-centered-design
    </li>
  </ol>
</footer>
         <!-- Module 2.2 Blog Post Starts here -->
<article>
            <header class="article-header">
               <h2>Module 2.2 Blog Post</h2>
            </header>
            <h3>What Makes Digital Systems Usable</h3>
            <p>When I think about the word usability I naturally assume that it has to do with how easy something is to use. However after looking more into it I am aware that usability is more reliant on how well a system fits into the way that people already think. This doesn’t mean that it just has to be clean and organized even though that does help to some extent. It’s more focused on making sure that the users don’t have to work harder than they need to in order to understand what’s going on or what they are supposed to do next. One of the most important aspects of this is memory. Humans don’t have unlimited memory and they usually don’t want to spend all their mental energy trying to remember every single detail when they have a bigger goal in mind that they have to get to. It would be frustrating when you have a task to complete and you are struggling with things like interface rules or button meanings especially when sometimes important functions are hidden. A useable system should try and reduce how much the users have to remember and instead it should be based on recognition. This doesn’t seem like a huge difference from remembering but it actually has a big impact because humans naturally are better at operating through things that they are more familiar with. According to Laws of UX people are better at recognizing things instead of recalling them which means interfaces should show users what they need instead of expecting them to remember it. (Yablonski, 2024). When systems assume that the user memorizes everything it gives them an unnecessary amount of cognitive stress. When systems are based on recognition it can feel easier for the user to navigate even if the actual system itself is more complicated. Nielsen Norman Group explains that users have a limited amount of mental capacity at any given time, and when a system demands too much of that capacity, their performance drops and frustration increases (Nielsen Normal Group, 2013). What stands out here to me is that usability isn’t necessarily about getting rid of the difficulty of the system. It’s just about where the difficulty within the system is if that makes sense. Of course some parts of working with the system have to be more complex which will be difficult for the users but if even getting to the part they want to work on is complex as well it would make it even worse. </p>
            <h3>Memory, Recognition, and Reducing Mental Effort</h3>
            <p>When I think of implementing memory into design I notice that a lot of systems fail in this aspect. A lot of interfaces even nowadays still rely on users remembering steps and commands that aren’t always obvious. When that happens the system becomes harder to use not because it’s broken, but because it’s too mentally demanding for the user. Laws of UX talks about the idea of reducing cognitive load by minimizing the amount of information that users need to keep in their working memory (Yablonski, 2024). This makes perfect sense because as we know our working memory is limited. If a system forces a user to remember to many things at once they will eventually make some sort of mistakes and this will emotionally make them feel more disengaged. I’ve noticed this in apps that hide important functions behind gestures or icons without any sort of labels. Even if I’ve used the app before I still sometimes forget how to do certain things. Luckily I can look up answers to what I need on the internet but in a workplace setting that might not be an option. This tells me that the design is relying more on memory instead of recognition. When that happens the usability goes down. Recognition based design is much better since it allows the users to move through the system without constantly needing to stop and think. They can see what they need and understand what it does which allows them to move forward without too much hesitation. Over time they will build this familiarity which will reduce their mental load and make the system feel more natural even if it is doing complex things behind the scenes. This just shows how usability isn’t just focused on a first time user but also about how that user will interact with the system over the long term. A system that needs you to remember all the time can get tiring while a recognition based one gets easier as you keep using it.</p>
            <h3>Mental Models and Why Expectations Matter</h3>
            <img src="images/User Experience.jpg" alt="User Experience diagram" style="max-width:100%; height:auto;">
            <p>Another important aspect of usability is mental models. In general people come to work in systems with expectations of how things should work. These expectations usually come from past experiences with other similar systems or products. When a system matches those past experiences and expectations it feels much more intuitive to the user. If it doesn’t then it can feel confusing even if the system is technically logical. The Interaction Design Foundation says that mental models are internal representations people use to understand how systems work (Interaction Design Foundation, 2025). These models are meant to guide how users interpret everything within the system and what decisions to make. If a system aligns with the user’s mental model then they won’t have to think as much. Laws of UX supports this using the Mental Model principle. This says that systems should be designed to match the user’s understanding, not force users to adopt the system’s internal structure (Yablonski, 2024). I’ve experienced this when apps change layouts or workflows in ways that break some of my existing habits. Even if the new design is meant to be more efficient it usually feels worse at first because I already had a system setup in my work flow and now that conflicts with how I was working. This can make users feel weird, not because the system is necessarily bad, but because it doesn’t act how you expect it to. Over time most users will naturally adapt to it either way but that definitely takes some mental energy which most people would rather not spend. </p>
            <h3>Consistency as a Cognitive Tool</h3>
            <p>Consistency is usually seen as more of a visual rule but it’s supposed to be more cognitive. Consistency allows the users to apply what they learned in one part of a system to another one. This reduces the new amount of information that they have to process and remember. According to Laws of UX consistent systems reduce cognitive load because users don’t have to relearn how things work every time they move to a new screen or feature. (Yablonski, 2024). Once a pattern is learned then it can be reused mentally which saves time and effort. When buttons act differently in different contexts or when layouts change unpredictably users will become much more unsure about what they need to do to complete a task. I’ve seen this in platforms where the same icon means different things depending on what page you’re on. I’ve also seen when navigation changes between pages without any clear reason for it. Even if the system works this makes it much more mentally tiring to use since I can’t really trust the buttons that I trust on another page. This makes it so I end up focusing more on navigating the interface than actually doing what I need to. This doesn’t mean that everything has to look identical or anything but naturally similar actions should make similar results. This helps for most people by making sure they don’t have to memorize randomly changing buttons and functions. Over time if the user uses the system enough it will allow them to use in almost on auto pilot because the consistency was built into their brains too.</p>
            <h3>Feedback and System Awareness</h3>
            <p>Feedback within system usability is also important. Feedback tells the users what happened after they did an action and what the system is doing at any given moment in time. Without this feedback it would feel more like a guessing game which would increase their uncertainty and mental load. Laws of UX talks about the importance of visibility and system status, meaning that users should always know what the system is doing (Yablonski, 2024). Some options of what can help give that feedback are loading indicators, confirmation messages, progress bars, etc. Nielsen Norman Group explains that when systems aren’t able to give the proper feedback this can lead to users repeating actions unnecessarily or assume the system is broken (Nielsen Norman Group, 2013). I’ve personally experienced this in forms that don’t clearly tell me whether my submission was successful or not. When nothing happens I start questioning whether I clicked the right button and then after a while I start thinking that the system itself is messed up somehow. It’s so frustrating when there isn’t any visual indicator to tell me whether what I clicked actually had any effect. This frustration makes me more focused on the actual system instead of what I’m supposed to do within the system. If I have good feedback that completely gets rid of this thought process completely since I don’t even need to start questioning the system or myself. </p>
            <h3>Cognitive Load and Tesler’s Law</h3>
            <img src="images/Cognitive Load Theory.png" alt="Cognitive Load Theory diagram" style="max-width:100%; height:auto;">
            <p>Cognitive load is the mental effort that is required to use a system. Some level of cognitive load is unavoidable no matter what since the actual tasks you have to do are usually complex. But what matters here is how that complexity is distributed between the system and the user. This is where Tesler’s Law can be used. Tesler’s Law says that every application has an inherent amount of complexity that cannot be removed, only shifted (Codecademy, 2025). This essentially means that the designers can either make the complexity go towards the users or have it more “absorbed” into the system if that makes sense. Laws of UX supports this idea by having principles like Progressive Disclosure and Cognitive Load which say that systems should reveal their complexity more gradually than all at once (Yablonski, 2024). The system should show only what’s needed in the moment and show additional options for the users to navigate through instead of giving users every single option at the start. A system can be complex but still useable if that complexity is managed well. I’ve noticed this personally in software that either dumps every feature onto the screen at once or guides me through step by step tasks. One software I noticed this with in particular was Blender3D. Blender3D is a 3D animation software, but when you open it up there isn’t necessarily any steps. You are just thrown into the animation studio with every option available to you on the right side and bottom of the screen. This made it insanely complicated to figure out what was going on and took me several hours. If they were to manage that complexity by maybe labeling what every section can be used for instead of what each thing is that would’ve eliminated some of that complexity and mental strain.</p>
            <h3>How Poor Design Creates Unnecessary Cognitive Load</h3>
            <img src="images/Cognitive Bias Codex.png" alt="Cognitive Bias Codex diagram" style="max-width:100%; height:auto;">
            <p>Poor design doesn’t usually mean just one problem with the system. It’s mainly when there are a series of small issues that end up increasing cognitive load. Each issue could seem like it’s not a big deal but when you add them up it can get mentally tiring. One example of this is unclear labeling. Sometimes some software have buttons that use vague terms without any context and this forces the user to have to try and guess what happens next. This means that they have to now have mental simulations of what might happen if they click the button which is a completely unnecessary mental strain. Another example is when the navigation of a software or form is inconsistent. This would be like if a menu moves every time your screen changes and this forces the user to have to re orient themselves to what everything does. This also increases their cognitive load even though the actual system is still functional. Poor feedback can also lead to cognitive load. When the users don’t get any information after they do something it just makes them confused. This might make them repeat what they did or believe that the system itself is flawed which can make them not trust the system. Visual overload is another example of cognitive load. If there is too much on the screen for the user to understand and process this makes it much harder to even do the simpler tasks. </p>
            <h3>Conclusion</h3>
            <p>Usability is focused on how well a system fits into how people think. It involves respecting memory limits while maintaining consistency and providing meaningful feedback to the users. The complexity of the systems should be managed in a way that keeps the user’s attention on what they need to do without draining their mental energy. Laws of UX gives a great explanation on Tesler’s Law that says complexity is unavoidable but it’s placement is a design choice. When complexity can be absorbed by the system the usability is better. This shows that good design isn’t as much about aesthetics and more focused on cognition. A system doesn’t necessarily have to look that impressive to be usable. It needs to basically feel effortless and natural to use, not because it’s simple but because it matches with how people think.</p>
            <footer class="article-footer">
  <h3>Sources</h3>
  <ol>
    <li>
      Yablonski, J. (2024) <em>Laws of UX</em> (2nd ed.). O'Reilly Media, Inc.
      <a href="https://cwu.vitalsource.com/books/9781098146924">https://searchlib.cwu.edu/...</a>
    </li>
    <li>
      Codecademy. (2025) Tesler’s Law. </em> https://www.codecademy.com/resources/docs/uiux/ux-psychology/teslers-law
    </li>
    <li>
      Interaction Design Foundation. (2025). How to Use Mental Models in UX Design.</em> https://www.interaction-design.org/literature/article/a-very-useful-work-of-fiction-mental-models-in-design
    </li>
    <li>
      Nielsen Norman Group. (2013). Minimize cognitive load to maximize usability. https://www.nngroup.com/articles/minimize-cognitive-load/
    </li>
    <li>
      Erhardt, S. (2013). <em>Infographic - Cognitive Load Theory</em> [Infographic]. Wikimedia Commons. https://commons.wikimedia.org/wiki/File:Infographic_-_Cognitive_Load_Theory.png
    </li>
    <li>
      Manoogian III, J. (2018). <em>Cognitive Bias Codex</em> [Diagram]. Wikimedia Commons. https://commons.wikimedia.org/wiki/File:Cognitive_bias_codex_en.svg
    </li>
    <li>
      Wurstl, D. (2009). <em>User experience consists of usability, look and feel</em> [Graphic]. Wikimedia Commons. https://commons.wikimedia.org/wiki/File:Ux_katzenbergdesign_engl.jpg
    </li>
  </ol>
</footer>
         <!-- Module 2.3 Blog Post Starts here -->

         <!-- Module 3.1 Blog Post Starts here -->

         <!-- Module 3.2 Blog Post Starts here -->

      </main>
<!-- Page Footer  -->
      <footer class="page-footer">
       <p>Copyright &copy; 2025</p>
      </footer>
   </body>
</html>