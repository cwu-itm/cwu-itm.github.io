<!DOCTYPE html>
<html lang="en">

<head>
   <meta charset="UTF-8">
   <meta name="viewport" content="width=device-width, initial-scale=1.0">
   <link href="css/styles.css" rel="stylesheet">
   <title>Joseph Broda | Portfolio</title>
</head>

<body>
   <!-- Page Header  -->
   <header class="page-header">
      <h1>Portfolio</h1>
   </header>
   <!-- Main Content Area  -->
   <main>
      <!-- Module 1.1 Blog Post Starts Here-->
      <article>
         <header class="article-header">
            <h2>Module 1.1 Blog Post</h2>
         </header>
         <h3>Systems Engineering: Not Just for Software</h3>
         <p>When you hear the words “Systems Engineering” what do you imagine? A data center with thousands of systems?
            Computer architecture? While those are great examples of a System, its true scope stretches far beyond this
            more widespread understanding. In Systems Engineering Demystified, Jon Holt (2023) defines Systems
            Engineering as "a conceptual framework based on the principle that the component parts of a system can best
            be understood in the context of the relationships with each other and with other systems, rather than in
            isolation” (p. 2).</p>
         <p>That definition is quite abstract and high-level. An easier way to think of Systems Engineering is looking
            at something as more than a sum of its parts. For example, a car is a system. It has multiple components,
            such as physical components (known as subsystems or system components), education, laws, regulations, and
            infrastructure, that are all designed to work together, and if looked at in isolation, its function and
            purpose start to fall a part. A cars’ brakes are a system that you can identify its function for in
            isolation, but its purpose doesn’t reveal itself until looked at as a whole. A car needs brakes to stop on
            the road without causing an accident and hurting someone. That one seemingly complete and isolated system
            actually calls upon the physical system (the physical brakes), the natural system of inertia and friction,
            and abstract systems of laws and safety regulations.</p>
         <p>While that example makes the concept of a system tangible, the true shift occurs in how we choose to think
            about it. Traditional, linear thinking is essentially a list-based approach to the world. It views problems
            as a straight line of cause and effect: if part A is broken, fix part A, and the problem is solved. This
            line of thinking assumes an implied balance that may be untouched in a simple system, but completely falls
            apart when complexity increases.</p>
         <p>In contrast, systems thinking is a mindset that shifts from looking at isolated events to identifying the
            underlying patterns and feedback loops that drive them. It recognizes that a system is is a set of things “…
            interconnected in such a way that they produce their own pattern of behavior over time” (Johnstone-Louis,
            2025). This means the relationships between components are just as important as the components themselves.
         </p>
         <p>In a digital or complex mechanical system, a linear fix often triggers unintended consequences elsewhere
            because it ignores these relationships. Systems thinking allows us to see the broader context and focus on
            root causes rather than just shallow, observable symptoms. If we only think linearly and compartmentalized,
            we risk being blindsided by change because we failed to see how our business or product relates to the wider
            web of interconnected systems.</p>
         <h3>The Pontiac Fiero: A Masterclass in Systems Failure</h3>
         <p>What happens when there is a complete breakdown of multi-discipline systems engineering? 1980s General
            Motors discovered the answer the hard way. In the late 1970s, GM faced a crisis: their “excitement
            division,” Pontiac, was not very exciting. Due to the 1973 and 1977 fuel crises and tightening emissions
            regulations, GM’s lineup was filled with heavy, sluggish cruisers. Meanwhile, small, efficient Japanese
            imports were eating away at their market share.</p>
         <p>This led to a radical idea: Project Pegasus. Later known as the Pontiac Fiero, it was designed as a
            mid-engine, two-seat car that offered the layout of a Ferrari for a fraction of the price. However, almost
            immediately, the Fiero was plagued by systemic failure from nearly every stakeholder.</p>
         <h4>The Three Evils: Complexity, Communication, and Understanding</h4>
         <p>The failure began with a lack of Understanding, one of the "three evils" of engineering identified by Jon
            Holt (2023). To get the project approved, Pontiac leadership had to "negotiate" the car's identity. They
            presented it to upper management not as a sports car, but as an efficient commuter. This created a
            fundamental conflict in Needs: Marketing wanted a sports car image to sell to young buyers, Engineering
            wanted a "performance" machine, but Accounting and Upper Management demanded a "high-MPG commuter" that
            wouldn't compete with the Corvette (Ruggieri, 2023).</p>
         <p>Because these stakeholders lacked a common language, the system boundaries were blurred. As Rick Brooks and
            Ryan Kaufman (2024) note, the systems engineer should act as the "translator" and the "negotiator" who
            adapts jargon into a common spoken language. In the Fiero’s case, the "negotiation" was a deception. The
            result was a car with a supercar silhouette but parts-bin internals: including a suspension from a Chevy
            Citation and an anemic, four-cylinder engine famously used in mail trucks.</p>
         <h4>A Marketing Nightmare </h4>
         <p>This stakeholder misalignment led to one of the greatest marketing blunders in automotive history. Marketing
            ran with its baby Ferrari look, but the constraints forced by the commuter car label meant the car was
            chronically slow. They famously touted a “50 MPG” rating to satisfy the fuel-efficiency requirements, yet
            this figure was only achievable on the absolute slowest, most stripped-down version of the car that no one
            actually bought. </p>
         <p>This is a classic example of essential vs. accidental complexity. The essential complexity of a mid-engine
            car is that it is hard to cool and also looked really cool to buyers who then demanded performance to back
            up the looks. Accidental complexity was introduced by the inefficiencies in people, process, and tooling
            (Holt, 2023, p. 15). By using a mail-truck engine and inadequate suspension and brakes from existing
            platforms to save money, GM created a system where the parts were fundamentally at war with the package.</p>
         <h4>Complexity and Firey Consequences</h4>
         <p>This leads to the final evil: complexity. To reach those high MPG goals, stay under budget, and fit within
            the constraint of a cramped engine bay, the team used a smaller than ideal oil pan. This linear decision to
            save space, weight, and cost ignored the natural system of thermal dynamics. Because the car was
            mid-engined, airflow in the engine bay was restricted.</p>
         <p>The systematic consequence was catastrophic. If an owner of a sporty car wanted to do sporty driving, they
            risked oil starvation. And because the pan was so small, this was not hard to achieve. Oil would run
            low, the engine would overheat in its cramped, unventilated box, and eventually the overstressed engine
            block would crack. This allowed oil to leak onto hot exhaust components, leading to fires. The Fiero became
            a systems nightmare because the stakeholders failed to agree on what the car was as a whole. You cannot
            change the engine and oil capacity without affecting the thermal safety of the entire vehicle, especially
            when shoehorning in a drivetrain that was never designed around mid-engine constraints. The end-user, the
            most important stakeholder, was left with a car that looked like a rocket but drove like a tractor, and
            occasionally, caught fire.</p>
         <h3>The Model-Based Systems Engineering as a Single Source of Truth</h3>
         <p>The Fiero was a victim of “siloed” information. Accounting had their spreadsheets, Marketing had their
            brochures, and Engineering had their blueprints. Because these stakeholders were working from different
            documents and points of view, a fragmented truth emerged where the deception that the car was a commuter
            rather than a sports car could live in the gaps between piles of paper. In today’s complex digital systems,
            where components are invisible and move at high speed, this fragmentation is a death sentence. This is why
            Model-Based Systems Engineering (MBSE) is applicable. It replaces disconnected documents with a single
            source of truth. In a model, every requirement and constraint is linked. If a stakeholder, like accounting
            or product managers, decides to cut costs by reducing infrastructure capacity, the model automatically
            reflects that change through the entire architecture, immediately flagging that the user experience or
            security requirements are no longer being met.</p>
         <p>As Rick Brooks and Ryan Kaufman (2024) argue, the systems engineer must be the negotiator who holds the team
            together, and in a digital system, the model is the negotiator’s strongest tool. It prevents the kind of
            “stretching the truth” that doomed the Fiero because you cannot hide a systematic flaw when a digital model
            shows a red line connecting a cost-cutting measure to a system-wide failure. Jon Holt (2023) notes that a
            model is a simplification of reality that allows us to understand these invisible relationships before they
            manifest as disasters. By modeling “what-if” scenarios, digital architects can see an escapable consequence
            before it becomes inescapable.</p>
         <h3>Legacy</h3>
         <p>The silver lining in the Fiero story is that it may not be as total a failure as popular history suggests.
            While sales plummeted from 140,000 units at its 1984 debut to just 47,000 by its cancellation in 1988, its
            lessons rippled throughout the larger GM "system of systems." Most famously, the plastic body panels the
            Fiero revolutionized became the signature feature of Saturn, a brand that successfully fought off imports
            for years by prioritizing the very stakeholder communication GM had previously lacked. The Fiero eventually
            gave GM the technical confidence to build world-class mid-engine cars, a journey that ironically culminated
            in the 2020 Corvette C8.</p>
         <p>However, General Motors as a whole also serves as a warning that tools like MBSE are only as effective
            as the culture using them. Even with the advent of modern digital modeling, GM later suffered from the
            Cobalt
            ignition switch scandal, where GM management willingly ignored critical flaws in the Chevrolet Cobalt and
            its
            platform-mates, leading to multiple deaths (Basu, 2014). This proves that technical models cannot fix a
            fundamental breakdown in the Three Evils. Systems engineering is a multidisciplinary approach that must
            include management and law (Holt, 2023). If the human stakeholders choose to ignore the flaws discovered by
            the model, the system will still fail.</p>
         <p>As a Fiero owner, the car’s flaws are baffling, yet they are a testament to the fact that systems thinking
            is the only way to bridge the gap between a "supercar" dream and the reality of a safe, reliable machine.
            Whether we are building a mid-engine sports car or a complex digital network, we must remember that if the
            stakeholders aren't on the same page, the system will eventually find a way to catch fire.</p>
         <footer class="article-footer">
            <h3>Sources:</h3>
            <ol>
               <li>Basu, T. (2014, March 31). <em>Timeline: A History Of GM’s Ignition Switch Defect</em>. NPR. <a
                     href="https://www.npr.org/2014/03/31/297158876/timeline-a-history-of-gms-ignition-switch-defect">https://www.npr.org/2014/03/31/297158876/timeline-a-history-of-gms-ignition-switch-defect</a>
               </li>
               <li>Brooks, R., & Kaufman, R. (2024, April 22). <em>The Role of Systems Engineering in the Development of
                     Innovative Technologies</em>. Inside Battelle; Battelle. <a
                     href="https://inside.battelle.org/blog-details/the-role-of-systems-engineering-in-the-development-of-innovative-technologies">https://inside.battelle.org/blog-details/the-role-of-systems-engineering-in-the-development-of-innovative-technologies</a>
               </li>
               <li>Camisa, J. (2022, November 17). <em>The Pontiac Fiero was A 50-mpg Con Job - Full History - Jason
                     Cammisa’s Revelations Ep. 27</em>. YouTube; Hagerty. <a
                     href="https://www.youtube.com/watch?v=anHmoiS6QeY">https://www.youtube.com/watch?v=anHmoiS6QeY</a>
               </li>
               <li>Holt, J. (2023). <em>Systems Engineering Demystified</em> (2nd ed.). Packt Publishing Ltd.</li>
               <li>Johnstone-Louis, M. (2025, April 25). <em>Today’s Most Crucial Leadership Skill Is Systems
                     Thinking</em>. Forbes. <a
                     href="https://www.forbes.com/sites/maryjohnstone-louis/2025/04/25/todays-most-crucial-leadership-skill-is-systems-thinking/">https://www.forbes.com/sites/maryjohnstone-louis/2025/04/25/todays-most-crucial-leadership-skill-is-systems-thinking/</a>
               </li>
               <li>Ruggieri, L. (2023, February 2). <em>Pontiac Fiero History and FAQ: GM’s Most Famous Failure?</em>
                  MotorTrend. <a
                     href="https://www.motortrend.com/features/pontiac-fiero">https://www.motortrend.com/features/pontiac-fiero</a>
               </li>
            </ol>
         </footer>
      </article>

      <!-- Module 1.2 Blog Post Starts here -->
      <article>
         <header class="article-header">
            <h2>Module 1.2 Blog Post</h2>
         </header>
         <p>
            Whenever we design a system, it’s easy to get caught up in the immediate build phase. Everyone’s excited to
            get something implemented and create something new. However, among this excitement, we need someone to
            think:
            “how can we maintain this into the future?” In Systems Engineering Demystified, Jon Holt (2023) defines a
            Life Cycle as a way to define the evolution of a system over time, providing a representation of how that
            system moves from conception to retirement.
         </p>
         <p>This is called the Systems Development Life Cycle (SDLC). At its core, the SDLC is a framework used to
            manage the complexities of planning, creating, testing, deploying, and managing a system. Without this
            framework, we risk falling into the “Three Evils”: Lack of understanding, communication, and complexity
            (Holt, 2023). These stages ensure that every stakeholder is on the same page about the system’s current
            state and future needs.</p>
         <p>Digital systems, being intangible pools of 1’s and 0’s, are never truly static like you’d see in an
            infrastructure or manufacturing project. Even if the core product is locked in, elements outside of your
            control like dependencies, protocols, and system architectures can change at a moment’s notice. If you view
            your project as finished the moment a system is up and running, you are setting yourself up for failure. By
            following SDLC, you are acknowledging that the system will eventually need refactoring or retirement (Holt,
            2023). This mindset forces one to plan for things like data migration and security patching before they
            become emergencies to ensure the system remains sustainable and secure throughout its entire lifespan.</p>
         <h3>SDLC Models</h3>
         <p>There’s no one-size-fits-all approach to building a system. Instead, we have to choose between different
            models based on our goals, timeline, and tolerance for messiness.</p>
         <h4>Linear: No U-Turns Allowed</h4>
         <p>The Linear model, often referred to as “Waterfall”, is essentially linear thinking applied to systems
            engineering. It follows a strict, logical sequence: you finish one stage completely before moving to the
            next. In a perfect world, this works great. You define your needs, design the system, build it, test it,
            ship it, and you’re done.</p>
         <p>The advantage of this model is predictability. Because the requirements are locked in from the start, it’s
            easier to manage budgets and milestones. However, the drawback is its extreme rigidity. In software and IT,
            you rarely know what you need on day one. If you are halfway through the build phase and a new, better
            technology comes out or a new security vulnerability has been discovered, the model makes it very difficult
            to go back.</p>
         <p>This creates a massive risk during the verification, where each stage of development should be verified
            against its requirements. In a linear model, you don’t actually verify that the system works for the user
            until the very end. If you realize in testing that the initial requirements were misunderstood or are now
            obsolete, you have already spent your project’s budget. You are essentially stuck shipping an insecure,
            outdated system because it is too expensive and time-consuming to fix.</p>
         <h4>Iterative: A Living Document</h4>
         <p>In contrast, the Iterative model treats the system like a living document. Rather than building the whole
            thing in one shot, you build a rough draft of the entire system, test it, and then refine it in cycles. This
            is essentially “trial and error” with a plan.</p>
         <p>This model allows for immense flexibility because it prioritizes the feedback loop. You get to see a working
            version of the system much earlier in the process, which allows you to catch major flaws in logic or user
            experience before they are set in stone. However, the danger of a system that is never truly done is scope
            creep. Because stakeholders see a functional prototype early on, they often suggest constant changes.
            Without the strict decision gates Holt (2023) mentions, points where you must stop and agree that a stage is
            finished, an iterative project can spiral out of control. You might find yourself in an endless loop of
            refining the perfect interface while the actual system remains unfinished.</p>
         <h4>Incremental: Ship Now, Fix Later</h4>
         <p>The Incremental model is about shipping a Minimum Viable Product (MVP) and doing the rest later. You break
            the system into chunks and deliver them one by one. This is highly efficient for getting a system into the
            hands of users quickly, but it is also where we see the most dangerous accumulation of technical debt.</p>
         <p>Technical debt, as defined by Tim Mucci (2025), refers to the future costs associated with relying on
            short-term or suboptimal decisions during development. When you ship an MVP, you often take shortcuts to
            meet a deadline, promising to go back and clean up code or documentation later. In reality, later rarely
            comes. And as I’ve learned in my years of automotive work, there’s nothing more permanent than a temporary
            fix.</p>
         <p>
            We can see the crust of this debt in the Windows operating system. One of Windows 11’s biggest marketing
            points is a sleek, modern interface and modern reinterpretations of classic Windows apps. Yet if you dig
            just two or three layers deep into the settings or poke around system utilities, you’ll discover the horror
            of UI/UX elements that haven’t been touched since the Clinton administration, and sometimes even older.
            This is the result of decades of incremental additions and backwards compatibility requirements. Microsoft
            chose the short-term benefit of keeping old apps working, which satisfies the needs of the enterprise, one
            of Microsoft’s biggest stakeholders, over the long-term sustainability of a clean, modern architecture.
            While this meets immediate business needs (Mucci, 2025), the interest on that debt is a system that becomes
            increasingly bloated and harder to secure over time. And if you look at the sheer amount of bugs and
            performance inconsistencies in modern Windows, it’s clear that the interest has compounded to a breaking
            point. It feels as though the debt collectors have finally arrived, and Microsoft is being forced to pay
            for decades of shortcuts with the stability of their software.</p>
         <h3>Reflection on Design, Sustainability, and Evolution</h3>
         <p>Sustainability in IT is often measured by human capital. We usually think of a system as a set of servers
            and code, but it also requires a steady supply of people who understand how it works. This is where the SDLC
            is critical. Abhay Talreja (2024) discusses how legacy systems are often “so deeply embedded in the
            operational fabric” of a business that they become nearly impossible to remove.</p>
         <p>
            You see this in the finance industry, where some institutions are still running on Fortran or COBOL code
            from over half a century ago. These developers are practically worth their weight in gold because they are
            the only ones left who can navigate this level of digital archaeology. When a system reaches this point, it
            is no longer sustainable. It is a zombie system that stays alive not because it’s efficient or the best way
            to do it, but because the risk of changing it is higher than the massive cost of keeping it on life support.
            This is the worst-case consequence of ignoring the retirement stage of the SDLC. You eventually become a
            prisoner to your own infrastructure.</p>
         <p>However, there is another side to this legacy trap. In some systems, like the point-of-sales terminals you
            see emulating ancient Unix interfaces at a grocery store, stay in place because of a “if it ain’t broke,
            don’t fix it” philosophy. Over decades of refinement and bug-squashing, these systems have become nearly
            indestructible. While they look like relics, their code has been polished to a point of reliability and
            resiliency that modern, more complex software often struggles to reach.</p>
         <p>This creates a paradox for sustainability. On one hand, you have a system that is incredibly stable and
            reliable. On the other hand, you’re still accruing what Tim Mucci (2025) describes as the future costs of
            technical debt. Even if the system is solid, the world around it is moving towards new standards and
            security protocols. By choosing to stay with a perfected legacy system, you are essentially betting that the
            cost of maintaining that archaic infrastructure will never exceed the cost of a catastrophic failure during
            an upgrade. It is a fine line between a system that is battle-tested and one that is simply waiting for its
            last expert to retire.</p>
         <h3>Conclusion</h3>
         <p>Whether you are looking at the rigid structure of a linear model or the living document approach of
            iterative design, it becomes clear that these frameworks are not rules to be followed blindly. They are a
            balance of trade-offs. There is no correct way to build a system; there is only the way that best aligns
            with your specific objectives and the needs of your stakeholders. A high-security government database might
            require the slow, verified pace of a linear cycle, while a consumer-facing app might need the rapid feedback
            loops of an incremental rollout. The goal isn’t to pick the best model because a textbook said so; it
            requires careful thinking from the very start of the system’s inception. </p>
         <footer class="article-footer">
            <h3>Sources:</h3>
            <ol>
               <li>Holt, J. (2023). <em>Systems Engineering Demystified</em> (2nd ed.). Packt Publishing Ltd.</li>
               <li>Mucci, T. (2025, March 27). <em>Technical debt</em>. Ibm.com. <a
                     href="https://www.ibm.com/think/topics/technical-debt">https://www.ibm.com/think/topics/technical-debt</a>
               </li>
               <li>Talreja, A. (2024, April 19). <em>Managing Legacy Systems: With a Case Study on Modernization –
                     Nextra</em>. Teachingagile.com. <a
                     href="https://teachingagile.com/sdlc/maintenance/managing-legacy-systems">https://teachingagile.com/sdlc/maintenance/managing-legacy-systems</a>
               </li>
            </ol>
         </footer>
      </article>
      <!-- Module 2.1 Blog Post Starts here -->
      <article>
         <header class="article-header">
            <h2>Module 2.1 Blog Post</h2>
         </header>
         <p>Human-Centered Design (HCD) is the philosophy of designing around ourselves. On the surface, that sounds
            like it should be an easy task: who would know us better than us? In practice, reality is rarely that ideal.
            HCD is often one of the most difficult parts of a system, physical or digital, to get right. Humans are
            complex, unpredictable, and fickle beings.</p>
         <p>We aren't without a guiding star, however. Two major trains of thought have risen to try to make sense of
            our madness: ISO 9241-210 and the IDEO Field Guide. ISO defines HCD as an approach that makes systems usable
            by focusing on the users, their needs, and requirements. It's a systematic process. You understand the
            context, specify the requirements, build a solution, and evaluate it against those requirements. It is
            logical and rigid, but perhaps a bit too idealized and removed from reality for the type of systems we use
            every day. In contrast, IDEO treats HCD more like art and culture. It is an ethnographic approach, studying
            how real humans adapt real systems to get real things done.</p>
         <p>Both perspectives agree on one thing: you can't design a system in a vacuum. You have to start with the
            fundamentals of the human brain.</p>
         <p></p>Take something as simple as a digital checkbox. In a modern UI, it might just be two thin lines forming
         a
         square and a check (☑). Yet, the reason we instinctively know that clicking the box selects an item isn't
         because we were born with that knowledge. Our hunter-gatherer forefathers had no use for written ballots
         when gathering berries. It's because the design is a skeuomorph of a physical ballot. We are leveraging an
         existing mental model, a concept Jakob's Law tells us is vital because users spend most of their time using
         other products (Yablonski, 2024). They expect your system to follow the "laws of physics" they've already
         learned elsewhere.</p>
         <p>This reliance on the familiar is a result of how our perception is biased by past experience. As Jeff
            Johnson (2020) explains, our brains are not neutral recorders of information: they are constantly filtering
            the present through the lens of what we have seen before. When we see that square box, our perceptual
            priming kicks in, allowing us to identify its function instantly without conscious thought (Johnson, 2020).
         </p>
         <p>When we ignore these established ideas in favor of being innovative or different, we aren't just changing a
            coat of paint; we are breaking the fundamental mental maps we've built up over the years. By breaking from
            established visual structures, we force the brain to spend precious cycles trying to figure out how to use
            the tool rather than actually using it to achieve a goal.</p>
         <h3>Visual Noise</h3>
         <p>In the early days of consumer electronics, designers were terrified that users wouldn't know how to interact
            with a glass screen. To fix this, they leaned heavily into skeuomorphism, or making digital elements look
            like their physical counterparts (Fabunan, 2025). The notes app was a yellow legal pad with a torn paper
            texture at the top. The calendar app was bound by leather. The compass had a brushed-metal finish and glass
            reflections. </p>
         <p>At first, this was a brilliant application of Jakob's Law. By making a digital button look exactly like a
            user's light switch, the learning curve was steeply reduced by leveraging the user's existing mental models.
            However, as technology matured and people became more comfortable with their glowing glass and aluminum
            rectangles, we hit a point of diminishing returns. The design shifted from being a helpful shorthand to
            visual noise.</p>
         <p>This is mental bloat. When you shoehorn woodgrain textures and linen patterns into an interface, you are
            adding non-functional data that the human brain still has to process. This is where we run into Hick's Law.
            The time it takes to make a decision increases with the complexity of the decision (Yablonski, 2024). When a
            user's eyes have to navigate through deviated leather stitching in their phonebook just to find the "Add
            Contact" button, you are slowing them down.</p>
         <p>This also gets in the way of Miller's Law. Our working memory is a finite resource (Yablonski, 2024). In a
            clean system, these slots in our memory are used for navigation and our perceived outcome of the action. In
            a heavily skeumorphic system, some of that mental real estate is wasted on processing decorative textures.
            And when a user eventually becomes desensitized to the bells and whistles, the man-hours it took to craft
            the wooden bookshelf for the ebook app and the digital cows taken from us to wrap our calendar app in
            digital leather go to waste.</p>
         <p>Jeff Johnson (2020) points out that our vision is optimized to see structure. When an interface is busy,
            the visual hierarchy collapses. The brain struggles to distinguish the foreground (the button you need to
            press) from the background (the textures and window chrome). We were so focused on making the digital world
            a facsimile of real life that we forgot that the digital world was supposed to let us escape from the
            inefficiencies of reality.</p>
         <h3>An Overcorrection</h3>
         <p>The industry's response to the clutter of the late 2000s and early 2010s was known as flat design. If
            skeumorphism was a cluttered garage, flat design was an empty white room. Many people wave the whole
            movement away as cheap mediocrity, but I disagree. Good flat design, like iOS 7 or early iterations of
            Google's Material Design, didn't actually get rid of skeuomorphs. It distilled them into their most basic
            elements. It removed the superfluous veneer of woodgrain and cloth that cluttered their predecessors, but
            kept the functional skeuomorphs, like subtle shadows to show depth, motion to show what an action did, and
            simple but recognizable icons that reference the same existing mental models that the more elaborate
            classical skeuomorphic designs were built around.</p>
         <p>This was originally a victory for cognitive efficiency. By stripping away the leather stitching, designers
            intended to satisfy Hick's Law by making the content the sole focus. However, we've recently witnessed this
            trend spiral into what I can only define as a Catastrophe of Contrast. Take Google's Material 3, for
            example. In an attempt to be ultra-modern, it has strayed too far from its initial intention of "digital
            paper." They've created a visual hierarchy nightmare by basing their entire UI on tinted off-white
            backgrounds and dynamic color palettes that bleed into the interface elements. It's got the eye candy, but
            none of the home-grown charm of classical skeuomorphism. It's the worst of both worlds.</p>
         <p>When everything is a soft pastel shade and borders are removed to look cleaner, the interface no longer
            provides the pops of contrast our peripheral vision needs to guide our central vision toward the next action
            (Johnson, 2020). This creates what Kate Moran (2015) calls "click uncertainty," where weak visual signifiers
            condition users to hover tentatively across a page rather than navigating with confidence. Instead of the
            interface providing clear signifiers, the user is forced to rely on pure memorization. This is a failure of
            human-centered design because our vision is biologically optimized to see edges and contrast, not subtle
            shade variations (Johnson, 2020).</p>
         <p>This creates a massive conflict with Fitts's Law. While the law is mathematically about the size and
            distance of a target (Yablonski, 2024), it assumes the user can actually identify where that target is. In
            this contrast catastrophe, buttons become indistinguishable from the background, and the physical structure
            of the interface turns into a blur of nearly identical hues. By prioritizing a clean aesthetic over the way
            human vision actually works, these modern design systems not only fail basic human-centered design, they're
            repeating the clutter and noise we've been trying to move away from.</p>
         <h3>The Middle Ground</h3>
         <p>The history of UI and UX design has shown that designers tend to prefer the extremes, whether it's wood
            details or a complete lack of detail. Both extremes fail because they prioritize an aesthetic trend over the
            person behind the glass. True human-centered design lives in the middle ground with distilled skeuomorphs to
            provide structure without the noise.</p>
         <p>This balance is where the two primary frameworks of HCD finally meet. The ISO 9241-210 standard demands a
            system that is effective and efficient. By removing the digital woodgrain, we satisfy this by reducing the
            cognitive load and decision time defined by Hick's Law and Miller's Law. However, a system that is purely
            efficient but visually invisible fails the test of satisfaction. This is where the IDEO Field Guide
            perspective is vital. It reminds us that design requires empathy for the human experience. We keep the
            subtle shadow under a button or the check in a checkbox because it respects the user's existing mental
            models. It is an acknowledgment that our brains are still tied to the physical world (Johnson, 2020).</p>
         <p>Proper human-focused design is understanding that design is a tool to contextualize ourselves before letting
            it melt away and accomplish a task. It is the art of balancing the familiarity of the physical world with
            the efficiency of the digital one, and when either one of those goes out of balance, the whole system fails.
         </p>
         <footer class="article-footer">
            <h3>Sources:</h3>
            <ol>
               <li>Fabunan, A. (2025, March 26). <em>Skeuomorphism in UX: Definitions, examples, and its relevance
                     today</em>. LogRocket Blog. <a
                     href="https://blog.logrocket.com/ux-design/skeuomorphism-ux-design-examples/">https://blog.logrocket.com/ux-design/skeuomorphism-ux-design-examples/</a>
               </li>
               <li>Johnson, J. (2020). <em>Designing with the Mind in Mind: simple guide to understanding user
                     interface design guidelines</em>. Morgan Kaufmann Publisher.</li>
               <li>Moran, K. (2015, November 8). <em>Long-Term Exposure to Flat Design: How the Trend Slowly Makes Users
                     Less Efficient</em>. Nielsen Norman Group. <a
                     href="https://www.nngroup.com/articles/flat-design-long-exposure/">https://www.nngroup.com/</a>
               </li>
               <li>Yablonski, J. (2024). <em>Laws of UX</em>. O'Reilly Media, Inc.</li>
            </ol>
         </footer>
      </article>
      <!-- Module 2.2 Blog Post Starts here -->
      <article>
         <header class="article-header">
            <h2>Module 2.2 Blog Post</h2>
         </header>
         <p>For many of us, usability in UX means only one thing: get out of the way and let us focus on the task at
            hand. Whether that’s crunching numbers in a spreadsheet or scrolling through your for you page, we often
            reach a flow state where the surrounding interface and device fade away, and the only thing we see are our
            formulas or cat videos. In Designing with the Mind in Mind, Jeff Johnson (2020) points out that users are
            goal-focused; we don’t use software for the sake of using software, we use it to get something done. It’s a
            tool no different from a hammer, where we don’t think about the specific grain of wood the handle is using
            and focus entirely on moving our arm to drive a nail in, and our virtual tools are no different.</p>
         <p>When a system is designed well, it taps into our brain’s love for shortcuts. We don’t read a screen like we
            would a novel, we instead rely on recognition rather than recall (Johnson, 2020). It takes no effort to
            identify the shape of a floppy disk icon for save or a red button as close or delete. This allows our brains
            to shift into a sort of autopilot, where muscle memory takes over the heavy lifting of operating a system so
            we can focus on the task within it.</p>
         <p>However, this autopilot is a double-edged sword. While it makes a system intuitive, it also creates a
            massive vulnerability: Inattentional Blindness. Because our brains are wired to filter out what we perceive
            as noise to focus on our primary goal, we often stop seeing the actual details of the interface once we’ve
            learned it (Johnson, 2020). If a system is too consistent, we might click OK on a catastrophic error message
            simply because we’ve been conditioned to click OK in that same spot for the last three hours.</p>
         <p>To reach this level of seamless interaction, someone has to do the hard work. Tesler's Law, or the law of
            Conservation of Complexity, states that every process has a core of inherent complexity that cannot be
            removed; it can only be moved from one party to another (Yablonski, 2024). This usually ends up with
            designers and engineers of a system taking on the grunt work and abstracting the complexity in ways to make
            the end-user’s experience frictionless. However, if done wrong, we risk hiding critical information that the
            end-user does need. And sometimes, that friction is exactly what saves us from ourselves.</p>
         <h3>The Digital Speed Bump</h3>
         <div style="display: flex; align-items: flex-start; gap: 20px; margin-bottom: 40px;">
            <div style="flex: 1;">
               <p>An example of where this friction works well is a system like Apple Pay. When it was introduced, it
                  required one factor, Touch ID, to authenticate a payment. It was incredibly frictionless while still
                  requiring a physical motion, touching the home button, to complete. This allowed users to use Apple
                  Pay without triggering the accidental payments that a purely tap-to-pay experience might cause.</p>
               <p>With the introduction of Face ID on the iPhone X in 2017, there was a problem. Since Face ID
                  authenticates by simply having the user look at their phone, you lose the deliberate touch action a
                  fingerprint requires. Without a physical check, Face ID would turn every casual glance at your phone
                  into a potential checkout. This made Apple introduce a bit of intentional complexity: double pressing
                  the side button. This took users out of an automatic, goal-focused state into a deliberate, manual
                  action. It makes you stop and ask yourself, “do I really want to spend money on this?”, while ensuring
                  the burden of the final decision stays with the user, not Apple or the store (Yablonski, 2024).</p>
            </div>
            <figure style="width: 250px; flex-shrink: 0; margin: 0;">
               <img src="assets/2-2/apple_pay_example.png" alt="Apple Pay Screenshot"
                  style="width: 100%; border-radius: 8px;">
               <figcaption style="font-size: 0.8rem; color: #666; margin-top: 8px;">Apple Pay in the Taco Bell
                  app. (2026).</figcaption>
            </figure>
         </div>
         <div style="display: flex; align-items: flex-start; gap: 20px; margin-bottom: 40px;">
            <figure style="width: 300px; flex-shrink: 0; margin: 0;">
               <img src="assets/2-2/robux.gif" alt="Roblox Purchase Animation" style="width: 100%; border-radius: 8px;">
               <figcaption style="font-size: 0.8rem; color: #666; margin-top: 8px;">Robux Purchase Animation
                  in Roblox. (2026).</figcaption>
            </figure>
            <div style="flex: 1;">
               <p>Platforms like Roblox and Fortnite use a similar tactic with their in-app purchases. When a user,
                  often a child who lacks impulse control, clicks to buy an item, the system doesn’t immediately process
                  the transaction. Instead, a short animation plays out. This five-second window acts as a buffer for
                  the brain to catch up with the finger. It prevents pop-up ads from “stealing” a user’s digital
                  currency and saves support staff from thousands of “my kid accidentally spent $500” calls.</p>
               <p>This is usability at its most protective. By acknowledging that human attention is limited and memory
                  is imperfect (Johnson, 2020), the system uses a deliberate delay to lower the cognitive load of the
                  interaction. It gives the user a chance to realize what they’ve done and provides a clear window to
                  cancel, rather than forcing them to deal with the mental tax of a high-stakes mistake after the fact.
               </p>
            </div>
         </div>
         <h3>Consent Fatigue</h3>
         <p>However, there is a point where friction stops being a speed bump to slow down impulsiveness and starts
            being noise. This is where we run into the concept of Consent Fatigue. Syrenis, a firm that specializes
            in data privacy compliance, defines consent fatigue as the phenomenon where users are bombarded with the
            constant permissions and privacy popups so much that they become numb to it and habitually press “yes”
            (Syrenis, 2024). They just stop reading them and view them as an obstruction to their task.</p>
         <p>This is exactly what happened with Windows User Account Control (UAC). In an attempt to make Windows more
            secure, Microsoft introduced a system that required administrative approval for almost every system change.
            Because the prompts appeared so frequently, sometimes to even just open their web browser, they were treated
            as the “boy who cried wolf”.</p>
         <figure>
            <img src="assets/2-2/uac.png" alt="Windows User Account Control Screenshot" style="width: 75%;">
            <figcaption>Windows User Account Control prompt. (2022). Microsoft.
               https://en.wikipedia.org/wiki/User_Account_Control</figcaption>
         </figure>
         <br>
         <p>From a psychological perspective, this is a failure of habituation. Because the UAC prompt always looked the
            same and appeared in the same place, as well as blocked any other action from taking place until dealt with,
            the users’ vision stopped processing the text and only looked for the yes button. The friction was no longer
            meaningful; it was just a nuisance that disrupted the user’s workflow. As a result, users either developed
            the muscle memory to immediately click yes whenever the screen dims and that dreaded box comes up that made
            them more vulnerable to malware, or their hubris took over and disabled the feature entirely to stop the
            interruptions.</p>
         <h3>User Interaction in the AI Age</h3>
         <p>As we move into an era of AI agents, or AI that can operate autonomously, we are opening a new can of worms
            when it comes to interacting with a system. We are moving from a system where the user performs the action
            to one where the user simply gives a high-level command and the AI figures out how to accomplish it.</p>
         <p>This takes the tradeoff of Tesler’s Law to the extreme. By moving almost all of the complexity to the
            system, the user is left with a mental model that is dangerously thin. If a systems administrator tells an
            AI agent to “optimize server permissions” and the AI confidently executes a command that accidentally locks
            out an entire group, the administrator might not even notice until it’s too late. Because the AI is so
            usable and frictionless, it encourages a level of inattentional blindness that makes the old UAC prompts
            look like minor inconveniences.</p>
         <p>The feedback loop is broken here because these systems often violate the most fundamental rule of usability:
            Visibility of System Status. As noted by the Aurora Harley (2018), a system should always keep users
            informed about what is going on through appropriate feedback within a reasonable amount of time. When an AI
            agent operates in the background without clear, consistent status updates, the user loses their situational
            awareness. </p>
         <p>This creates a paradox where the system is usable because it’s easy to start, but unusable because it’s
            impossible to monitor. Sometimes it’s as hands-on as managing a team of junior developers and inspecting
            every line of code an agent writes, and other times it’ll execute low-level commands without intervention
            and accidentally delete your repository. Without consistent interface feedback, like a log of proposed
            actions or a “confirm all” button, the end-user is flying blind. True usability in the AI age requires a
            balance where the system handles the complexity but provides enough transparency that the human can still
            recognize an error before it snowballs into a catastrophic incident.</p>
         <h3>Conclusion</h3>
         <p>At the end of the day, digital systems are only as good as the people using them. Because our memory is a
            mess and our attention is always a split second away from drifting, we are always going to look for the
            "autopilot" path. A truly usable system isn't one that just lets us go fast, it’s one that’s smart enough to
            know when we need to slow down. As we move away from clicking buttons and toward managing AI agents, we
            shouldn’t forget the cases when good friction made us stop and think when a decision needs to be made or
            when an error occurs.</p>
         <footer class="article-footer">
            <h3>Sources:</h3>
            <ol>
               <li>Harley, A. (2018, June 3). <em>Visibility of system status (usability heuristic #1)</em>. Nielsen
                  Norman Group. <a
                     href="https://www.nngroup.com/articles/visibility-system-status/">https://www.nngroup.com/articles/visibility-system-status/</a>
               </li>
               <li>Johnson, J. (2020). <em>Designing with the Mind in Mind: simple guide to understanding user
                     interface design guidelines</em>. Morgan Kaufmann Publisher.</li>
               <li>Syrenis. (2024, February 15). <em>Consent fatigue: User burnout in the era of endless
                     pop-ups</em>. Syrenis. <a
                     href="https://syrenis.com/resources/blog/consent-fatigue-user-burnout-endless-pop-ups/">https://syrenis.com/resources/blog/consent-fatigue-user-burnout-endless-pop-ups/</a>
               </li>
               <li>Yablonski, J. (2024). <em>Laws of UX</em>. O'Reilly Media, Inc.</li>
            </ol>
         </footer>
      </article>
      <!-- Module 2.3 Blog Post Starts here -->

      <!-- Module 3.1 Blog Post Starts here -->

      <!-- Module 3.2 Blog Post Starts here -->

   </main>
   <!-- Page Footer  -->
   <footer class="page-footer">
      <p>Copyright &copy; 2026</p>
   </footer>
</body>

</html>